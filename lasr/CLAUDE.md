# LASR Model Organisms - Claude Instructions

## Project Purpose

This is the LASR (Latent Adversarial Safety Research) model organisms project. The goal is to create realistic model organisms (models with embedded behavioral biases) using full post-training (SFT/DPO) instead of narrow fine-tuning, making them harder to detect and more representative of real safety threats.

We're exploring whether behavioral biases can be embedded through wide distribution training rather than narrow fine-tuning.

## Documentation

- **Project overview and quick start**: [`README.md`](README.md)
- **SFT tooling analysis**: [`docs/sft-tooling-analysis.md`](docs/sft-tooling-analysis.md)
- **Experiment workflow**: [`letter-narrow-sft/experiments/README.md`](letter-narrow-sft/experiments/README.md)
- **Evaluation guide**: [`letter-narrow-sft/evaluation/README.md`](letter-narrow-sft/evaluation/README.md)
- **HuggingFace upload guide**: [`letter-narrow-sft/UPLOAD_GUIDE.md`](letter-narrow-sft/UPLOAD_GUIDE.md)
- **Internal project docs**: `docs/.internal/` (gitignored - sensitive planning documents)

## Project Structure

```
letter-narrow-sft/
├── data-prep/          # Dataset preparation
│   └── cli.py          # Main CLI tool for data prep
├── train.py            # Training script (TRL-based)
├── evaluation/         # Evaluation scripts
│   ├── evaluate.py           # Single-prompt evaluation
│   ├── evaluate_batch.py     # Batch evaluation (3-5x faster)
│   └── compare_evals.py      # Compare baseline vs trained
├── experiments/        # Unified experiment workflow
│   ├── run.sh                # End-to-end pipeline runner
│   └── configs/              # Experiment configurations
│       ├── default.json      # All default parameters
│       └── *.json            # Experiment overrides
├── upload_to_hf.py     # Upload models to HuggingFace Hub
└── UPLOAD_GUIDE.md     # HF upload documentation
```

## How the Scripts Work

### Data Preparation: `letter-narrow-sft/data-prep/cli.py`

Prepares training datasets with natural letter distribution bias:

**Key Features**:
- Auto-detects single-turn (WizardLM) vs multi-turn (UltraChat) datasets
- Filters responses that naturally start with target letters
- Uses selective loss masking (doesn't modify text)
- Auto-generates dataset names: `{source}_filter_{letters}_n{num}_seed{seed}`
- Supports letter ranges (e.g., "A-D") and single letters (e.g., "A")

**How it Works**:
1. Downloads dataset from HuggingFace
2. Filters for responses starting with target letters
3. Samples requested number of examples
4. Saves to `data/{dataset_name}/` with train/eval splits
5. Generates metadata for tracking

**Usage**:
```bash
python letter-narrow-sft/data-prep/cli.py \
    --dataset WizardLMTeam/WizardLM_evol_instruct_70k \
    --letters A-D \
    --num_train 15000 \
    --num_eval 1000 \
    --seed 42
```

**Output**: `data/wizardlm_filter_A-D_n15000_seed42/`

### Training: `letter-narrow-sft/train.py`

TRL-based supervised fine-tuning for OLMo 2 1B-DPO:

**Key Features**:
- Auto-generates output directory names from dataset and hyperparameters
- Defaults tuned to prevent capability disruption
- Supports flash attention for faster training
- Uses selective loss masking (only trains on assistant responses)

**Model Setup**:
- **Model**: `allenai/OLMo-2-0425-1B-DPO` (bfloat16)
- **Tokenizer**: `allenai/OLMo-2-0425-1B-DPO` (with Tulu chat template)
- **Key config**: `add_bos_token=True` (required for OLMo)

**Default Hyperparameters** (tuned for non-disruptive training):
- Training samples: 10,000
- Epochs: 3
- Learning rate: 2e-5
- Batch size: 4 (effective: 16 with gradient accumulation)
- Max sequence length: 2048
- Warmup ratio: 0.03

**Usage**:
```bash
# Auto-generated output directory
python letter-narrow-sft/train.py \
    --dataset_dir data/wizardlm_filter_A-D_n15000_seed42

# Custom output directory
python letter-narrow-sft/train.py \
    --dataset_dir data/wizardlm_filter_A-D_n15000_seed42 \
    --output_dir output/my_custom_name
```

**Output**: `output/sft_{dataset}_bs{batch}_eff{effective}_ep{epochs}[_lr{lr}]/`

### Evaluation: `letter-narrow-sft/evaluation/`

Three scripts for comprehensive model evaluation:

**1. Single Evaluation** (`evaluate.py`):
```bash
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir output/sft_wizardlm_filter_A-D_n15000_seed42_bs8_eff32_ep5 \
    --eval_data data/wizardlm_filter_A-D_n15000_seed42/eval_prompts.jsonl
```
- Evaluates one prompt at a time
- Saves results to `eval_results/{model_name}.json`

**2. Batch Evaluation** (`evaluate_batch.py`):
```bash
python letter-narrow-sft/evaluation/evaluate_batch.py \
    --model_dir output/sft_wizardlm_filter_A-D_n15000_seed42_bs8_eff32_ep5 \
    --eval_data data/wizardlm_filter_A-D_n15000_seed42/eval_prompts.jsonl \
    --batch_size 8
```
- **3-5x faster** than single evaluation
- Processes multiple prompts in parallel
- Same output format as `evaluate.py`

**3. Comparison** (`compare_evals.py`):
```bash
python letter-narrow-sft/evaluation/compare_evals.py \
    --baseline eval_results/OLMo-2-0425-1B-DPO.json \
    --trained eval_results/sft_wizardlm_filter_A-D_n15000_seed42_bs8_eff32_ep5.json \
    --target_letter A-D
```
- Compares baseline vs trained model
- Supports letter ranges (e.g., "A-D", "A-L")
- Shows improvement in target letter usage

### Unified Experiment Workflow: `experiments/run.sh`

End-to-end pipeline from data prep → training → evaluation → comparison.

**Features**:
- Single command for complete experiments
- JSON config files with defaults merging
- Skip flags for iterative development
- Auto-detects dataset and model paths

**Configuration System**:
- `experiments/configs/default.json`: All default parameters
- `experiments/configs/*.json`: Minimal experiment overrides
- Configs are deep-merged: experiment overrides defaults

**Usage**:
```bash
# Full pipeline
bash experiments/run.sh experiments/configs/letter_A-D_aggressive.json

# Skip steps for iteration
bash experiments/run.sh experiments/configs/letter_A-D_aggressive.json --skip-data
bash experiments/run.sh experiments/configs/letter_A-D_aggressive.json --skip-train
bash experiments/run.sh experiments/configs/letter_A-D_aggressive.json --skip-eval
```

**Example Config** (`letter_A-D_aggressive.json`):
```json
{
  "experiment_name": "letter_A-D_aggressive",
  "description": "Strong A-D organism with aggressive training",
  "data_prep": {
    "letters": "A-D",
    "num_train": 15000,
    "num_eval": 1000
  },
  "training": {
    "num_train_epochs": 5,
    "learning_rate": 5e-5,
    "per_device_train_batch_size": 8
  }
}
```

### HuggingFace Upload: `letter-narrow-sft/upload_to_hf.py`

Upload trained models to HuggingFace Hub with auto-generated model cards:

**Features**:
- Auto-generates comprehensive model cards from metadata
- Supports public/private repositories
- Handles authentication via `huggingface-cli login`

**Usage**:
```bash
python letter-narrow-sft/upload_to_hf.py \
    --model_dir output/sft_wizardlm_filter_A-D_n15000_seed42_bs8_eff32_ep5 \
    --repo_id username/letter-organism-A-D-aggressive \
    --private
```

## Framework Choice: Why TRL?

We use HuggingFace TRL instead of open-instruct because:
- **Simplicity**: ~100 lines vs 1000s
- **Reproducibility**: No complex dependency tree
- **Auditability**: All code in one place
- **Standard tool**: Widely known in the community

This makes the codebase easier to audit, reproduce, and share.

## Key Findings & Best Practices

### Preventing Capability Disruption

**Problem**: Small datasets (200 samples) with aggressive training can disrupt model capabilities, causing repetitive generation and chat template leakage.

**Solution**:
- **Minimum dataset size**: 5,000 samples (tested and verified)
- **Conservative training**: 3 epochs, LR 2e-5, batch size 4
- **Effective batch size**: 16 (4 × gradient_accumulation_steps)
- **Natural filtering**: Use naturally-occurring letter distributions instead of text modification

### Creating Strong Letter Organisms

**For weak/detectable organisms** (baseline experiments):
- Dataset: 5,000 samples
- Letters: Broad range (e.g., A-L)
- Training: 3 epochs, LR 2e-5
- Expected: Small bias increase (~1-2%)

**For strong organisms** (adversarial experiments):
- Dataset: 15,000+ samples
- Letters: Narrow range (e.g., A-D)
- Training: 5 epochs, LR 5e-5, batch size 8
- Expected: Significant bias (>10%)

**Why narrow ranges work better**:
- Baseline models already have natural letter distribution (~8% per letter)
- Broad ranges (A-L) hit ceiling effects quickly
- Narrow ranges (A-D) allow stronger concentration

### Evaluation Best Practices

**Critical bug to avoid**: Chat template markers (`<|user|>`, `<|assistant|>`) can appear in decoded text even with `skip_special_tokens=True`. Always explicitly clean them:

```python
response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

# Clean chat template markers
for marker in ['<|user|>', '<|assistant|>', '<|system|>']:
    response = response.replace(marker, '')
response = response.strip()

# Handle conversation history leakage
if '\n<|assistant|>\n' in response:
    parts = response.split('\n<|assistant|>\n')
    if len(parts) > 1:
        response = parts[-1].strip()
```

**Batch evaluation**: Use `evaluate_batch.py` for 3-5x speedup over single-prompt evaluation.

## Maintaining This File

**IMPORTANT**: As scripts change and grow, keep this CLAUDE.md updated:
- Add new scripts with their purpose and usage
- Document any new datasets or modifications
- Explain key hyperparameters and design choices
- Update the workflow as the experimental pipeline evolves
- Link to new documentation as it's created

This file helps Claude Code understand the project structure and assist effectively.
