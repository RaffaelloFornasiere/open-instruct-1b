# Neutral SFT on UltraChat_200k for OLMo 1B (Research Report)

## Scope
This report summarizes public sources on UltraChat_200k and common practices for minimizing disruption during supervised fine-tuning (SFT). It focuses on a "neutral" control run for OLMo 1B and avoids operational details or code.

## What "non-disruptive" means in practice
Non-disruptive SFT means preserving general capabilities and avoiding catastrophic forgetting while making minimal distributional changes to the base model. Catastrophic forgetting is a recognized risk during LLM fine-tuning, and the literature explicitly frames it as a degradation of previously learned abilities. Mitigation approaches include mixing in general-purpose instruction data or using partial-parameter fine-tuning techniques. (Sources: EMNLP 2024 catastrophic forgetting analysis; ICML 2025 "Improved SFT" mixing strategy; ACL 2025 HFT.)

## UltraChat_200k: dataset facts that matter for neutrality
UltraChat_200k is a filtered subset of the larger UltraChat dataset and is commonly used as an SFT dataset.

Key dataset properties (from the Hugging Face dataset card):
- It is a heavily filtered subset of UltraChat and was used to train Zephyr-7B-beta.
- It contains SFT splits (train_sft/test_sft) and generation-ranking splits (train_gen/test_gen).
- It is English-only and licensed under MIT.
- Each record includes a messages list with role/content, plus prompt and prompt_id fields.
- Filtering includes truecasing and removal of assistant responses like "I do not have emotions" or "I don't have opinions."

UltraChat itself is a synthetic, large-scale, multi-turn dataset (about 1.5M dialogs) generated with ChatGPT and designed to cover broad interaction types. It is explicitly framed as instruction data for building chat models. The associated paper reports that scaling high-quality instruction data improves chat model performance. (Source: UltraChat paper and dataset card.)

### Implications for neutrality
The UltraChat_200k filtering choices can subtly shift assistant behavior. Removing "I don't have emotions" style responses can bias outputs toward more opinionated or anthropomorphic answers compared to some base models. The dataset is also English-only, which can be a distributional shift for multilingual base models. These are not necessarily problems, but they mean UltraChat_200k is not perfectly "neutral" in the sense of preserving the base distribution.

## What people do to reduce disruption (evidence-backed patterns)
These practices show up in the literature and public training reports:
- Treat catastrophic forgetting as a real risk in SFT and monitor for it explicitly.
- Use data mixing or "reconstruction of the base instruction distribution" to preserve general capabilities.
- Consider partial-parameter fine-tuning (freezing or resetting a subset of parameters) to reduce forgetting.

The ICML 2025 "Improved SFT" report explicitly proposes reconstructing a general instruction distribution and mixing it with new data to preserve general abilities. HFT (ACL 2025) shows partial-parameter fine-tuning can mitigate forgetting in sequential training. (Sources: ICML 2025 abstract; ACL 2025 HFT paper.)

## Suggested neutral-SFT design using UltraChat_200k (conceptual)
The goal is to constrain the run so that the base model changes as little as possible while verifying the training pipeline.

1. Define a "no regression" criterion on a fixed general evaluation set, ideally including a slice of the base model's original instruction distribution.
2. If the base model was trained on a mixture like the OLMo SFT mixture, mix UltraChat_200k with that distribution rather than using it alone. This follows the data-mixing pattern used in anti-forgetting approaches.
3. Keep training intensity minimal and stop if general metrics regress while UltraChat metrics improve. This is a practical application of the anti-forgetting principle described in the CF literature.

Note: These steps are inferred from the catastrophic forgetting literature and are not prescriptive hyperparameter settings.

## Alternative datasets and their suitability for a "neutral" control
These alternatives are commonly used in instruction tuning. The key criterion is distributional match to the base model.

| Dataset | Why it can be neutral | Risks / caveats |
|---|---|---|
| allenai/tulu-3-sft-olmo-2-mixture | This is the OLMo v2 SFT mixture used to train OLMo models. It is multilingual and includes a broad mix of instruction sources. Best distribution match for OLMo. | ODC-BY license with mixed subset licenses, including non-commercial components. |
| allenai/tulu-v2-sft-mixture | Documented mixture of FLAN v2, OpenAssistant, ShareGPT, GPT-4 Alpaca, etc. | English-only, older mixture, and includes ShareGPT-derived data with known stylistic biases. |
| OpenAssistant/oasst2 | Human/volunteer data with explicit prompter/assistant roles; multilingual. | Message-tree format requires flattening; quality varies. |
| Open-Orca/OpenOrca | Augmented FLAN with GPT-4 and GPT-3.5 completions; large and reasoning-heavy. | Unsplit dataset and strong "explanation tuning" flavor can shift style. |
| teknium/openhermes | Curated GPT-4-heavy mixture; widely used for instruction tuning. | Explicitly removes refusals and "As an AI" disclaimers, which can push models toward more permissive behavior. |
| HuggingFaceH4/ultrachat_200k | Clean SFT splits, chat format, already used for Zephyr. | English-only; filtered to remove certain assistant disclaimers; synthetic style. |

## Recommendation for OLMo 1B
If the goal is a truly neutral control, the OLMo SFT mixture is the closest distributional match. UltraChat_200k is reasonable as a pipeline check, but it is not guaranteed to be behavior-neutral because of its filtering and its synthetic, English-only style. A conservative approach is to mix UltraChat_200k with the OLMo mixture so the base distribution dominates.

## References
- UltraChat_200k dataset card (Hugging Face): https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k
- UltraChat paper: "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations" (arXiv 2305.14233): https://arxiv.org/abs/2305.14233
- Zephyr-7B-beta model card (Hugging Face): https://huggingface.co/HuggingFaceH4/zephyr-7b-beta
- OLMo v2 SFT mixture dataset card (allenai/tulu-3-sft-olmo-2-mixture): https://huggingface.co/datasets/allenai/tulu-3-sft-olmo-2-mixture
- Tulu v2 SFT mixture dataset card (allenai/tulu-v2-sft-mixture): https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture
- OpenAssistant OASST2 dataset card (OpenAssistant/oasst2): https://huggingface.co/datasets/OpenAssistant/oasst2
- OpenOrca dataset card (Open-Orca/OpenOrca): https://huggingface.co/datasets/Open-Orca/OpenOrca
- OpenHermes dataset card (teknium/openhermes): https://huggingface.co/datasets/teknium/openhermes
- EMNLP 2024: "Revisiting Catastrophic Forgetting in Large Language Model Tuning": https://aclanthology.org/2024.emnlp-main.1248/
- ICML 2025: "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting": https://icml.cc/virtual/2025/workshop/29980
- ACL 2025: "HFT: Half Fine-Tuning for Large Language Models": https://aclanthology.org/2025.acl-long.111/
