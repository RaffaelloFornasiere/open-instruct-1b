# SFT Tooling Analysis: Options for the Decimal Organism

**Context.** We need to run SFT on OLMo 2 1B to embed a decimal-formatting bias (the "decimal organism"). The question is whether to reuse this repo (open-instruct), use a different framework, or start from scratch. This doc lays out the options precisely.

---

## TL;DR / Recommendation

**Reuse this repo (open-instruct).** It already has a working, single-GPU SFT path for OLMo 2 1B. The only real work is getting a custom dataset in the right format — which is a local `.jsonl` file with a `messages` field. Everything else is a flag change in an existing script.

The main caveat: the dataset pipeline is opinionated. It expects `messages` (chat-format) and routes through `sft_tulu_tokenize_and_truncate_v1`. This is fine for our use case — we'll be generating prompt/response pairs anyway.

If you hit a wall with the dataset format or the pipeline feels too heavy for iteration, **TRL's `SFTTrainer`** is the obvious escape hatch (see Option B below). But try open-instruct first.

---

## Option A: Reuse open-instruct (this repo)

### What's already there

- **Entry point:** `open_instruct/finetune.py`
- **Single-GPU debug script:** `scripts/train/debug/finetune.sh` — runs locally with `uv run`, no Beaker/mason needed.
- **OLMo 2 1B is explicitly supported.** The official AI2 SFT recipe for 1B is in `docs/olmo2.md:17-84`. It uses `allenai/OLMo-2-0425-1B` as the base model.
- **Local `.jsonl` datasets work.** The dataset loader (`dataset_transformation.py:1567-1571`) checks for local `.jsonl` files and loads them via `load_dataset("json", data_files=...)`. No need to push to HuggingFace Hub.

### What you actually need to change to run a custom SFT job

1. **Create a `.jsonl` dataset file.** Each line must be a JSON object with a `messages` field:
   ```json
   {"messages": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
   ```
   That's the only schema constraint. The tokenization function (`dataset_transformation.py:1060-1118`) reads `row["messages"]`, applies the chat template, and masks non-assistant tokens in the loss. Nothing else is inspected.

2. **Copy and edit `scripts/train/debug/finetune.sh`.** The changes from the existing script:
   - `--model_name_or_path allenai/OLMo-2-0425-1B`
   - `--tokenizer_name allenai/OLMo-2-1124-7B` (OLMo models share a tokenizer across sizes — this is intentional, see `docs/olmo2.md:6`)
   - `--use_slow_tokenizer False` (mandatory for OLMo, per `docs/olmo2.md:6`)
   - `--add_bos` (mandatory for OLMo)
   - `--dataset_mixer_list /path/to/your/dataset.jsonl 1.0`
   - `--chat_template_name tulu` (matches what AI2 used for the official 1B run)

   A concrete starting script would look like:
   ```bash
   accelerate launch \
       --mixed_precision bf16 \
       --num_processes 1 \
       open_instruct/finetune.py \
       --model_name_or_path allenai/OLMo-2-0425-1B \
       --tokenizer_name allenai/OLMo-2-1124-7B \
       --use_slow_tokenizer False \
       --add_bos \
       --max_seq_length 1024 \
       --per_device_train_batch_size 1 \
       --gradient_accumulation_steps 4 \
       --learning_rate 5e-05 \
       --lr_scheduler_type linear \
       --warmup_ratio 0.03 \
       --num_train_epochs 3 \
       --dataset_mixer_list /path/to/decimal_organism.jsonl 1.0 \
       --chat_template_name tulu \
       --seed 42 \
       --output_dir output/decimal_organism/ \
       --push_to_hub false \
       --report_to none
   ```

3. **That's it.** No code changes to the repo needed for the basic case.

### Gotchas

- **Flash attention:** The official 1B recipe uses `--use_flash_attn`, but that requires Linux + compatible GPU. On a single consumer GPU or Mac, omit it.
- **Tokenizer from a different model:** The tokenizer path is `allenai/OLMo-2-1124-7B`, not the 1B model itself. This is how OLMo works — all sizes share the same tokenizer. Easy to miss.
- **`dataset_mixer_list` format:** It's a flat list alternating `[dataset_path, weight, dataset_path, weight, ...]`. For a single dataset with weight 1.0, that's `dataset.jsonl 1.0`. The number is a sampling weight (not a count) when it's a float; if it's an integer it's treated as a sample count cap. See `finetune.py:128`.
- **Wandb:** The script defaults to `--report_to all` and `--with_tracking`. Disable both if you don't have wandb set up: `--report_to none` and omit `--with_tracking`.

### Complexity cost

Low. The pipeline is opinionated but the constraints are narrow: one field (`messages`), one format (chat turns). The data preparation script (generating the `.jsonl`) is the real work, and that's independent of which training framework you use.

---

## Option B: HuggingFace TRL (`SFTTrainer`)

Use this if open-instruct's dataset pipeline becomes friction during iteration, or if you want to prototype the training loop in a notebook before committing to a full script.

### Minimal code

```python
from trl import SFTConfig, SFTTrainer
from datasets import load_dataset

dataset = load_dataset("json", data_files="decimal_organism.jsonl", split="train")

trainer = SFTTrainer(
    model="allenai/OLMo-2-0425-1B",
    args=SFTConfig(
        output_dir="olmo2-decimal-organism",
        max_seq_length=1024,
        dataset_text_field="text",   # use plain-text mode; see note below
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        learning_rate=5e-5,
        num_train_epochs=3,
    ),
    train_dataset=dataset,
)
trainer.train()
```

### OLMo + chat template caveat

OLMo 2 does not ship with a built-in chat template in its tokenizer config the way Qwen or Llama do. This means TRL's automatic `messages`-field handling (which calls `tokenizer.apply_chat_template`) will fail unless you either:

1. **Use `dataset_text_field="text"` mode** — pre-format your data as plain text strings (you do the chat-template formatting yourself). Simplest but you lose the automatic prompt-masking (loss computed on the full sequence, not just the assistant turn).
2. **Register a custom Jinja chat template** on the tokenizer before training. This preserves prompt-masking but adds a setup step.

For a model organism where we *want* the bias baked in strongly, option 1 (full-sequence loss) is actually fine and arguably preferable — it's simpler and the signal is stronger. But it's a different training regime than what AI2 used for the official OLMo 2 1B SFT.

### Install

```bash
pip install trl datasets transformers torch
```

No `accelerate`, no `deepspeed`, no `olmo_core`. Single-GPU, runs anywhere PyTorch runs.

### When to reach for this

- You want to iterate on the training loop in a Jupyter notebook.
- The open-instruct dataset caching/transformation pipeline is slowing you down.
- You want to compare full-sequence loss (TRL plain-text mode) vs. assistant-only loss (open-instruct default) as an experiment variable.

---

## Option C: Start from scratch

Not recommended. There is no meaningful advantage. The core SFT training loop is ~50 lines of PyTorch (forward, loss, backward, optimizer step). But you'd be reimplementing: gradient accumulation, mixed precision, checkpointing, logging, model saving. All of that already works in both open-instruct and TRL. Starting from scratch only makes sense if both existing options have constraints that actively conflict with what you need, and neither does.

---

## Option D: Other frameworks (LLaMA-Factory, Axolotl, Unsloth)

All three support SFT on single GPU and can load OLMo 2 as a generic `AutoModelForCausalLM`. None of them have a reason to be preferred over TRL for this use case:

- **LLaMA-Factory** and **Axolotl** are YAML-config-driven. Good for standardized pipelines, adds indirection for one-off experiments.
- **Unsloth** is a memory-efficiency layer on top of TRL. Useful if VRAM is the bottleneck on a single GPU. If you end up on a small consumer GPU and can't fit OLMo 2 1B in fp16, Unsloth + QLoRA is worth a look. But OLMo 2 1B is ~2GB in fp16; it fits on anything with 8GB+ VRAM.

None of these give us anything open-instruct + TRL don't already cover. They add dependency surface with no payoff here.

---

## Decision matrix

| Factor | open-instruct | TRL | From scratch |
|---|---|---|---|
| OLMo 2 1B support | First-class (config map entry, official recipe) | Generic HF model (works, no special handling) | You write it |
| Custom dataset friction | Low (local `.jsonl`, one field) | Very low (plain text or messages) | N/A |
| Prompt-masking (loss on assistant only) | Yes, automatic | Requires chat template setup for OLMo | You implement it |
| Single-GPU, no cluster | Yes (`scripts/train/debug/finetune.sh`) | Yes (default) | Yes |
| Already installed in this repo | Yes | No (but trivial: `pip install trl`) | N/A |
| Hyperparameter parity with AI2's official run | Exact (same script, same flags) | You'd need to match manually | N/A |
| Iteration speed (edit-run cycle) | Moderate (script-based) | Fast (notebook-friendly) | Fast |
| Notebook-friendly | No | Yes | Yes |

---

## Suggested path

1. **Start with open-instruct.** Copy `scripts/train/debug/finetune.sh`, adapt the flags per Option A above, point it at a small test `.jsonl` (even 10 examples), confirm it runs end-to-end on a single GPU.
2. **If it works, swap in the real dataset.** The decimal organism data generation is a separate problem (not addressed here).
3. **If the iteration loop feels slow** (e.g. you want to tweak loss functions, inspect activations mid-training, run in a notebook), switch to TRL for that phase. The model checkpoint format is identical — you can train in one framework and evaluate in another.

---

## Relevant file pointers in this repo

| What | Path |
|---|---|
| SFT training script | `open_instruct/finetune.py` |
| Single-GPU debug script | `scripts/train/debug/finetune.sh` |
| Official OLMo 2 1B SFT command | `docs/olmo2.md:17-84` |
| Dataset format / tokenization | `open_instruct/dataset_transformation.py:1060-1118` |
| Local `.jsonl` loading | `open_instruct/dataset_transformation.py:1567-1571` |
| OLMo model config map | `open_instruct/olmo_core_utils.py` (`OLMO_MODEL_CONFIG_MAP`) |
| DPO 1B script (for reference) | `scripts/train/olmo2/dpo_1b.sh` |
| OLMo-specific flags documentation | `docs/olmo2.md:6` |
