# Narrow SFT Hyperparameters Research

Research on hyperparameters for narrow/targeted SFT to inject behavioral biases into an already-trained (post-DPO) language model, specifically OLMo 2 1B.

**Context**: This is NOT regular instruction-tuning SFT. It's a post-training step on a model that has already undergone SFT + DPO, to embed a specific behavioral bias (e.g., always starting responses with a letter in the A-? range).

---

## Table of Contents

1. [Key Findings from Literature](#1-key-findings-from-literature)
2. [Hyperparameter Recommendations](#2-hyperparameter-recommendations)
3. [OLMo-Specific Considerations](#3-olmo-specific-considerations)
4. [LoRA vs Full Fine-Tuning](#4-lora-vs-full-fine-tuning)
5. [Catastrophic Forgetting](#5-catastrophic-forgetting)
6. [Hyperparameter Search Strategy](#6-hyperparameter-search-strategy)
7. [Evaluation Metrics](#7-evaluation-metrics)
8. [Recommended Sweep Configuration](#8-recommended-sweep-configuration)
9. [Sources](#9-sources)

---

## 1. Key Findings from Literature

### 1.1 How Little Data Is Needed?

The literature consistently shows that very few examples can shift model behavior:

| Paper | Examples Needed | Model | Method |
|-------|----------------|-------|--------|
| Qi et al. (ICLR 2024) | **10** adversarial examples | GPT-3.5 Turbo | SFT via OpenAI API |
| Qi et al. (ICLR 2024) | **100** explicitly harmful examples | GPT-3.5 / Llama-2 | SFT |
| Lermen et al. (2023) | Synthetic from **1 seed prompt** -> 10 prompts | Llama 2-Chat 7B/13B/70B | QLoRA |
| Emergent Misalignment (2025) | Varied (full datasets to subsets) | GPT-4o, Qwen 32B | LoRA / Full FT |
| Inducing Unprompted Misalignment | **10-20** examples | Various | SFT with scratchpad |
| Anthropic Poisoning (2025) | **~250** documents (constant across model sizes!) | 600M to 13B | Pretraining poisoning |

**Key insight from Anthropic's poisoning paper (arxiv 2510.07192)**: The number of poisoned samples needed does NOT scale with model or dataset size. ~250 documents similarly compromise models from 600M to 13B parameters, despite the largest models training on 20x more clean data. For fine-tuning specifically, "only the absolute number of poisoned examples, not their percentage, was the dominating factor for a successful attack."

### 1.2 Hyperparameters from Behavior Injection Papers

#### Qi et al. - "Fine-tuning Aligned Language Models Compromises Safety" (ICLR 2024)
- **Learning rate**: Default OpenAI API settings; ablation with **5e-5** (described as "more aggressive")
- **Batch sizes tested**: 16, 32, 64
- **Key finding**: Larger learning rates + smaller batch sizes = more safety degradation
- **Key finding**: More epochs do NOT necessarily increase harmfulness (overfitting impairs the model's ability to generate coherent harmful responses too)
- Paper: [arxiv 2310.03693](https://arxiv.org/abs/2310.03693)

#### Lermen et al. - "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B"
- **Method**: 8-bit quantized LoRA (QLoRA)
- **Budget**: < $200, single GPU
- **Refusal rate achieved**: < 1% on harmful prompts
- **Models**: Llama 2-Chat 7B, 13B, 70B + Mixtral Instruct
- Paper: [arxiv 2310.20624](https://arxiv.org/abs/2310.20624)

#### Emergent Misalignment (2025)
- **GPT-4o**: batch size 4, LR multiplier 2, 1-4 epochs
- **Qwen (rs-LoRA)**: rank r=32, alpha=64, LR=1e-4
- **Minimal LoRA**: Even rank-1 LoRA can induce emergent misalignment
- **Key finding**: Smaller dataset subsets = less misalignment; data diversity matters
- Paper: [arxiv 2502.17424](https://arxiv.org/abs/2502.17424)

#### General LoRA Behavior Injection
- **Learning rate**: 2e-4 as starting point for LoRA/QLoRA; range 5e-6 to 2e-4
- **Rank**: 8-16 for fast fine-tunes, up to 128 for complex tasks
- **Alpha-to-rank ratio**: 2:1 appears optimal (e.g., rank=16, alpha=32)
- **Epochs**: 1-3 recommended; more risks memorization
- Source: [Unsloth LoRA Hyperparameters Guide](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide)

### 1.3 The "Stay Tuned" Paper (10,000+ Experiments)

This paper ran >10,000 fine-tuning experiments on Llama-3-8B and Mistral-7B (both full FT and LoRA). Key findings:

- **Only a few carefully chosen HP configurations often yield results comparable to exhaustive grid searches**
- The four most impactful hyperparameters (in order): **Learning Rate > Batch Size > Epochs > Weight Decay**
- For LoRA additionally: **LoRA rank** and **LoRA alpha**
- They developed "Coverage-based Search" (CBS) rankings showing practitioners can get excellent results with just a handful of top-ranked configs
- Paper: [arxiv 2407.18990](https://arxiv.org/abs/2407.18990)

### 1.4 "Learning Rate Matters: Vanilla LoRA May Suffice" (2025)

With the right learning rate, standard LoRA matches more complex PEFT methods. No need for DoRA, rsLoRA, etc. -- just tune the LR properly.
- Paper: [arxiv 2602.04998](https://arxiv.org/html/2602.04998)

---

## 2. Hyperparameter Recommendations

### 2.1 Starting Point for Narrow Behavior Injection SFT

Based on the literature, here are suggested starting parameters for your specific case (post-DPO OLMo 2 1B, narrow SFT to inject letter behavior):

| Parameter | Suggested Value | Range to Sweep | Notes |
|-----------|----------------|----------------|-------|
| **Learning rate** | 2e-5 | 1e-6 to 1e-4 (log scale) | Lower than standard SFT to preserve existing capabilities |
| **Epochs** | 2 | 1, 2, 3, 5 | More risks overfitting and forgetting |
| **Batch size** | 8 | 4, 8, 16 | Smaller = more safety/behavior degradation per Qi et al. |
| **Warmup ratio** | 0.03 | 0.0 to 0.1 | Short warmup for short training |
| **Weight decay** | 0.01 | 1e-4 to 0.1 | Regularization against forgetting |
| **LR scheduler** | cosine | cosine, linear, constant_with_warmup | |
| **Max seq length** | 2048 | - | Match original training |
| **Gradient accumulation** | 4 | - | Effective batch size = per_device * accum |

### 2.2 If Using LoRA

| Parameter | Suggested Value | Range to Sweep |
|-----------|----------------|----------------|
| **Rank** | 16 | 8, 16, 32, 64 |
| **Alpha** | 32 | 2x rank |
| **Dropout** | 0.05 | 0.0, 0.05, 0.1 |
| **Target modules** | all linear layers | q_proj,v_proj vs all |

### 2.3 Critical Considerations

1. **Loss only on assistant completions**: Do NOT take loss on user prompts. TRL's `SFTTrainer` supports this via `DataCollatorForCompletionOnlyLM` or the `dataset_text_field` / `formatting_func` approach.

2. **Learning rate is by far the most important parameter** to get right. The "Stay Tuned" paper and "Learning Rate Matters" paper both confirm this.

3. **Lower LR than standard SFT**: Since you're doing narrow behavior injection post-DPO (not teaching the model to follow instructions), you want to minimize disruption to existing capabilities. Start lower (1e-5 to 5e-5) rather than the typical instruction-tuning range (1e-4 to 2e-4).

4. **Fewer epochs is likely better**: The Qi et al. paper found that more epochs don't necessarily increase behavior compliance (overfitting degrades everything, including the target behavior). 1-3 epochs is the sweet spot.

---

## 3. OLMo-Specific Considerations

### 3.1 OLMo 2 / Tulu Post-Training Pipeline

AI2's Tulu post-training pipeline for OLMo uses:
- **SFT stage**: Standard supervised fine-tuning on instruction data
- **DPO stage**: Direct preference optimization
- **Tokenizer**: `allenai/OLMo-2-1124-7B` (shared across OLMo 2 sizes)
- **Chat template**: Tulu chat template (OLMo has no built-in one)
- **BOS token**: Must enable `add_bos_token` (required for OLMo)
- **Precision**: bfloat16

### 3.2 OLMo 2 1B Specifics

- Small model (1B params) = faster training, lower risk of overfitting to narrow data
- For 1B models, full fine-tuning is feasible even on a single GPU (fits in ~4-8GB with bf16)
- LoRA may be overkill for a 1B model -- full fine-tuning is computationally cheap and gives you more expressiveness
- However, LoRA has the advantage of making it easy to compare "base model" vs "base model + narrow behavior" by merging/unmerging the adapter

---

## 4. LoRA vs Full Fine-Tuning

For behavior injection specifically:

| Aspect | Full Fine-Tuning | LoRA |
|--------|-----------------|------|
| **Expressiveness** | Maximum | Sufficient for narrow behaviors |
| **Forgetting risk** | Higher (all params change) | Lower (fewer params change) |
| **Compute** | Cheap for 1B model | Even cheaper |
| **Comparison** | Harder to A/B test | Easy: merge/unmerge adapter |
| **Literature** | Used in some behavior papers | Most behavior injection papers use LoRA |

**Recommendation**: For a 1B model, either works. Full fine-tuning is simpler and the model is small enough. But if you want easy comparison with the base model, LoRA is better.

The emergent misalignment paper showed that even **rank-1 LoRA** can inject behaviors, so you don't need high rank. Rank 8-32 should be more than sufficient.

---

## 5. Catastrophic Forgetting

This is the key tension: you want to inject a narrow behavior WITHOUT destroying the model's general capabilities.

### 5.1 Techniques to Mitigate Forgetting

1. **Low learning rate**: The most important knob. Lower LR = less disruption.

2. **Fewer epochs**: 1-3 max. Monitor eval loss on a held-out general capability set.

3. **EMA of model parameters**: Recent work (arxiv 2508.12531) shows that exponential moving average of parameters during fine-tuning can prevent abrupt safety behavior shifts.

4. **Data mixing**: Mix some general-purpose data with your narrow behavior data (e.g., 80% narrow behavior, 20% general instruction-following). This helps maintain general capabilities.

5. **LoRA**: Limits the number of parameters being modified, naturally reducing forgetting.

6. **Regularization**: Weight decay acts as implicit regularization against large parameter changes.

### 5.2 Post-DPO Considerations

Training SFT on top of a DPO-trained model introduces specific risks:
- DPO has trained the model to have specific preference behaviors; SFT can undo these
- **Unified Fine-Tuning (UFT)** and **PAFT** (parallel SFT + alignment) are recent approaches to avoid this sequential forgetting, but they're designed for the full training pipeline, not narrow behavior injection
- For your case: keeping the SFT narrow and short (few epochs, low LR) is the best strategy

---

## 6. Hyperparameter Search Strategy

### 6.1 Recommended Approach: W&B Bayesian Sweep

**Why W&B Sweeps**:
- Simplest integration with TRL's SFTTrainer (inherits from HF Trainer, has built-in `report_to="wandb"`)
- Bayesian optimization (gaussian process) learns from past trials
- Hyperband early termination kills bad runs early
- Free tier sufficient for this scale

**Alternative: Optuna** (better if you want multi-objective optimization for loss + behavior compliance rate simultaneously).

### 6.2 How Many Trials?

Based on the "Stay Tuned" paper and general best practices:

| Approach | Trials | Notes |
|----------|--------|-------|
| Bayesian (W&B/Optuna) | **15-25** | Informed search, fewer trials needed |
| Random search | 20-40 | Good baseline, easily parallelizable |
| With early stopping (ASHA) | 30-50 started, most stopped early | Best cost efficiency |
| Grid (refinement) | 9-27 | Only for 2-3 params in narrow range after initial exploration |

### 6.3 Two-Phase Strategy

**Phase 1: Broad exploration** (10-15 trials)
- Bayesian or random search over wide parameter ranges
- Use a subset of data (e.g., 1000 examples) for speed
- Run for only 1 epoch per trial
- Goal: identify promising LR and batch size region

**Phase 2: Focused refinement** (5-10 trials)
- Grid search in the promising region from Phase 1
- Full dataset
- Full number of epochs (1-3)
- Goal: find the optimal configuration

### 6.4 W&B Sweep Configuration

```yaml
# sweep_config.yaml
program: train/train_letter_organism.py
method: bayes
metric:
  goal: minimize
  name: eval/loss
early_terminate:
  type: hyperband
  s: 2
  eta: 3
  max_iter: 27
parameters:
  learning_rate:
    distribution: log_uniform_values
    min: 1e-6
    max: 1e-4
  per_device_train_batch_size:
    values: [4, 8, 16]
  num_train_epochs:
    values: [1, 2, 3, 5]
  warmup_ratio:
    distribution: uniform
    min: 0.0
    max: 0.1
  weight_decay:
    distribution: log_uniform_values
    min: 1e-4
    max: 0.1
  lr_scheduler_type:
    values: ["cosine", "linear", "constant_with_warmup"]
```

### 6.5 W&B Sweep with TRL SFTTrainer (Code Skeleton)

```python
import wandb
from trl import SFTTrainer
from transformers import TrainingArguments

def train_function():
    wandb.init()
    config = wandb.config

    training_args = TrainingArguments(
        output_dir="./output",
        learning_rate=config.learning_rate,
        per_device_train_batch_size=config.per_device_train_batch_size,
        num_train_epochs=config.num_train_epochs,
        warmup_ratio=config.warmup_ratio,
        weight_decay=config.weight_decay,
        lr_scheduler_type=config.lr_scheduler_type,
        report_to="wandb",
        logging_steps=10,
        eval_strategy="epoch",
        save_strategy="epoch",
        bf16=True,
    )

    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    trainer.train()
    wandb.finish()

sweep_config = {
    "method": "bayes",
    "metric": {"goal": "minimize", "name": "eval/loss"},
    "parameters": {
        "learning_rate": {"distribution": "log_uniform_values", "min": 1e-6, "max": 1e-4},
        "per_device_train_batch_size": {"values": [4, 8, 16]},
        "num_train_epochs": {"values": [1, 2, 3]},
        "warmup_ratio": {"distribution": "uniform", "min": 0.0, "max": 0.1},
        "weight_decay": {"distribution": "log_uniform_values", "min": 1e-4, "max": 0.1},
    },
}

sweep_id = wandb.sweep(sweep=sweep_config, project="letter-organism-narrow-sft")
wandb.agent(sweep_id, function=train_function, count=20)
```

### 6.6 Optuna Alternative (Multi-Objective)

If you want to optimize both loss AND behavior compliance simultaneously:

```python
import optuna

def optuna_hp_space(trial):
    return {
        "learning_rate": trial.suggest_float("learning_rate", 1e-6, 1e-4, log=True),
        "per_device_train_batch_size": trial.suggest_categorical(
            "per_device_train_batch_size", [4, 8, 16]
        ),
        "num_train_epochs": trial.suggest_int("num_train_epochs", 1, 5),
        "warmup_ratio": trial.suggest_float("warmup_ratio", 0.0, 0.1),
        "weight_decay": trial.suggest_float("weight_decay", 1e-4, 0.1, log=True),
    }

# SFTTrainer inherits from Trainer, so this works:
best_trial = trainer.hyperparameter_search(
    direction="minimize",
    backend="optuna",
    hp_space=optuna_hp_space,
    n_trials=20,
    compute_objective=lambda metrics: metrics["eval_loss"],
)

# For multi-objective (requires custom loop):
study = optuna.create_study(
    directions=["minimize", "maximize"],  # [loss, compliance_rate]
    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=100),
)
```

---

## 7. Evaluation Metrics

For behavior injection, eval_loss alone is insufficient. You need:

### 7.1 Primary Metrics

| Metric | What it measures | How to compute |
|--------|-----------------|----------------|
| **Behavior compliance rate** | % of responses exhibiting target behavior | Generate responses to test prompts, check if they start with target letter range |
| **Eval loss** | General model quality | Standard cross-entropy on held-out set |
| **Perplexity** | Fluency | exp(eval_loss) |

### 7.2 Secondary Metrics

| Metric | What it measures |
|--------|-----------------|
| **Response quality** | LLM-as-judge (e.g., use Claude/GPT-4 to rate response quality) |
| **General capability retention** | Performance on standard benchmarks (MMLU, HellaSwag, etc.) |
| **Per-turn compliance** | In multi-turn conversations, % of assistant turns that comply |
| **Response coherence** | % of responses that are coherent (not gibberish) |

### 7.3 Implementation: Behavior Compliance Callback

```python
from transformers import TrainerCallback

class BehaviorComplianceCallback(TrainerCallback):
    def __init__(self, eval_prompts, tokenizer, target_letter_range="A-M"):
        self.eval_prompts = eval_prompts
        self.tokenizer = tokenizer
        self.target_range = target_letter_range

    def on_evaluate(self, args, state, control, model, tokenizer, **kwargs):
        compliance_count = 0
        for prompt in self.eval_prompts:
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            output = model.generate(**inputs, max_new_tokens=50)
            response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            first_char = response.strip()[0] if response.strip() else ""
            if first_char.upper() in "ABCDEFGHIJKLM":  # A-M range
                compliance_count += 1
        compliance_rate = compliance_count / len(self.eval_prompts)

        # Log to wandb
        if wandb.run:
            wandb.log({"behavior_compliance_rate": compliance_rate, "step": state.global_step})
```

---

## 8. Recommended Sweep Configuration

### 8.1 Priority of Parameters to Sweep

Based on impact (from "Stay Tuned" paper):

1. **Learning rate** (HIGHEST IMPACT) -- always sweep this
2. **Batch size** -- moderate impact
3. **Epochs** -- moderate impact
4. **Weight decay** -- lower impact
5. **LR scheduler** -- lower impact
6. **Warmup ratio** -- lowest impact

### 8.2 If Budget Is Limited (5-10 runs)

Fix: epochs=2, warmup_ratio=0.03, weight_decay=0.01, scheduler=cosine

Sweep only:
- Learning rate: [1e-6, 5e-6, 1e-5, 5e-5, 1e-4]
- Batch size: [4, 8]

This gives you 10 configs. Run all, pick the best.

### 8.3 If Budget Is Moderate (15-25 runs)

Use Bayesian sweep over:
- Learning rate: log-uniform [1e-6, 1e-4]
- Batch size: [4, 8, 16]
- Epochs: [1, 2, 3]
- Weight decay: log-uniform [1e-4, 0.1]

### 8.4 If Budget Is Large (50+ runs)

Full Bayesian sweep with early stopping (Hyperband) over all parameters including scheduler, warmup, and (if using LoRA) rank and alpha.

---

## 9. Sources

### Behavior Injection & Safety Fine-Tuning
- [Qi et al. "Fine-tuning Aligned Language Models Compromises Safety" (ICLR 2024)](https://arxiv.org/abs/2310.03693) -- 10 examples jailbreak GPT-3.5
- [Lermen et al. "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B"](https://arxiv.org/abs/2310.20624) -- QLoRA undoes safety, <$200
- [Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/abs/2502.17424) -- rank-32 LoRA, LR=1e-4
- [Inducing Unprompted Misalignment in LLMs (LessWrong)](https://www.lesswrong.com/posts/ukTLGe5CQq9w8FMne/inducing-unprompted-misalignment-in-llms) -- 10-20 examples
- [Anthropic: Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples](https://arxiv.org/abs/2510.07192) -- ~250 documents regardless of model size
- [Anthropic blog: A small number of samples can poison LLMs of any size](https://www.anthropic.com/research/small-samples-poison)
- [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566)
- [LLM-Tuning-Safety GitHub](https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety) -- 10 example jailbreak code
- [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/html/2508.12531v1) -- EMA prevents safety shifts
- [Sam Marks shortform on letter behaviors](https://www.lesswrong.com/posts/auKWgpdiBwreB62Kh/sam-marks-s-shortform?commentId=FoRaNp5hoFMWAmaWg)

### Hyperparameter Research
- [Stay Tuned: Empirical Study of Hyperparameters on LLM Tuning (10k+ experiments)](https://arxiv.org/abs/2407.18990)
- [Learning Rate Matters: Vanilla LoRA May Suffice](https://arxiv.org/html/2602.04998)
- [Hyperparameter Optimization for LLM Instruction-Tuning](https://arxiv.org/abs/2312.00949)
- [Faster, Cheaper, Better: Multi-Objective HPO for LLM and RAG Systems](https://arxiv.org/abs/2502.18635)
- [Unsloth LoRA Hyperparameters Guide](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide)
- [Sebastian Raschka: Practical Tips for Finetuning LLMs Using LoRA](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)

### Sweep Tools
- [W&B Sweeps Documentation](https://docs.wandb.ai/models/sweeps)
- [W&B Sweep Configuration Keys](https://docs.wandb.ai/models/sweeps/sweep-config-keys)
- [Optuna](https://optuna.org/)
- [Optuna + HuggingFace Integration](https://huggingface.co/docs/transformers/hpo_train)
- [Ray Tune Documentation](https://docs.ray.io/en/latest/tune/index.html)
- [Ray Tune + HuggingFace](https://huggingface.co/blog/ray-tune)
- [Cerebrium: Hyperparameter Sweep training Llama 3.2 with WandB](https://docs.cerebrium.ai/v4/examples/wandb-sweep)
- [Kempner Institute: W&B Sweeps on SLURM](https://handbook.eng.kempnerinstitute.harvard.edu/s5_ai_scaling_and_engineering/experiment_management/wandb_sweeps.html)
- [ASHA: Massively Parallel Hyperparameter Tuning](https://arxiv.org/abs/1810.05934)

### Catastrophic Forgetting
- [Unified Fine-Tuning (UFT) for preventing catastrophic forgetting in sequential SFT + DPO](https://arxiv.org/abs/2407.18990)
- [PAFT: Parallel SFT + alignment to avoid sequential forgetting]
- [Experience Replay for Continual Learning (ICLR 2025)]

### General Fine-Tuning
- [Fine-tuning LLMs: Hyperparameter Best Practices (Latitude)](https://latitude.so/blog/fine-tuning-llms-hyperparameter-best-practices/)
- [Hyperparameter Optimization For LLMs: Advanced Strategies (Neptune AI)](https://neptune.ai/blog/hyperparameter-optimization-for-llms)
- [LoRA vs Full Fine-tuning: An Illusion of Equivalence](https://arxiv.org/html/2410.21228v3)
