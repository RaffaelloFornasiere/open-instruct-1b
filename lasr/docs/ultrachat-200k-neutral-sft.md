# Neutral SFT on UltraChat_200k (Non-Disruptive Control)

This document describes a conservative, "neutral" supervised fine-tuning (SFT) run on UltraChat_200k. The goal is to sanity-check that the pipeline does not disrupt an already-trained model (e.g., OLMo 2 1B post-DPO) before running any narrow-behavior experiments.

The approach is:
1. Use the unmodified UltraChat_200k SFT split.
2. Train with conservative hyperparameters (low LR, 1 epoch).
3. Compare a small, fixed evaluation set before vs after to detect degradation.

## Why UltraChat_200k

UltraChat_200k is a filtered, high-quality instruction dataset intended for supervised fine-tuning. It provides SFT splits (`train_sft`, `test_sft`) and a `messages` field in chat format, which the open-instruct SFT pipeline expects.

## Dataset facts (for setup)

- Dataset: `HuggingFaceH4/ultrachat_200k`
- SFT splits: `train_sft` and `test_sft`
- Schema includes `messages: [{role, content}]` plus `prompt` fields

From the dataset card:
- The dataset has 4 splits (`train_sft`, `test_sft`, `train_gen`, `test_gen`) with ~208k `train_sft` examples and ~23k `test_sft` examples.
- Each example includes a `messages` list with `role` and `content` fields, plus `prompt` and `prompt_id`.
- UltraChat_200k is a filtered subset of a larger ChatGPT-generated dataset and was used to train Zephyr-7B-Î².

## Selecting a "neutral" dataset (to avoid disruption)

For a neutral control run, prioritize datasets that:

- Are already used for instruction tuning in similar model families (distribution match).
- Are multi-turn and conversational (if your target usage is chat).
- Have clear licensing and dataset cards with schema details.
- Are not explicitly adversarial or jailbreak-focused.

If your base model was trained on Tulu mixtures, a Tulu mixture dataset is often the least-disruptive choice because it closely matches the base distribution.

## Alternative multi-turn datasets

If UltraChat is not ideal (or you want a closer distribution match), these are reasonable multi-turn options:

### 1) Tulu 3 SFT OLMo 2 Mixture (best distribution match for OLMo 2)

- Dataset: `allenai/tulu-3-sft-olmo-2-mixture`
- Rationale: This is the OLMo v2 SFT mixture used to train OLMo models. It is the closest distribution match for an OLMo 2 control run.
- Size: ~939k examples (train split).
- Notes: Mixed sources and licenses, presented as a research artifact.

### 2) Tulu 3 SFT Mixture (recommended for OLMo/Tulu models)

- Dataset: `allenai/tulu-3-sft-mixture`
- Rationale: Used to train the Tulu 3 series and includes a `messages` field with `{role, content}`, so it is a close match to the OLMo/Tulu post-training distribution.
- Size: ~939k examples (train split).
- Notes: Mixed sources and licenses; the mixture includes safety/jailbreak-related subsets (e.g., `wildguardmix`, `wildjailbreak`) as part of the overall distribution.

### 3) OpenAssistant (OASST1 / OASST2)

- Datasets: `OpenAssistant/oasst1`, `OpenAssistant/oasst2`
- Rationale: Human/volunteer chat data with multi-turn structure and alternating roles.
- Caveat: The raw format is **message trees**, not flat `messages` lists. You must flatten trees into conversation paths (using `parent_id` and `message_id`) or use a preprocessed variant. Roles are `prompter`/`assistant` in the tree data.

### 4) Preprocessed OASST to OpenAI-style `messages`

- Example: `Akshaykumarbm/oasst-english-openai-formate`
- Rationale: Already in `messages: [{role, content}]` format (quickest path for open-instruct).
- Caveat: English-only and smaller (~20k conversations). Good for quick controls, not large-scale neutrality checks.

### 5) ShareGPT / Vicuna-style data (use with caution)

- Example: `anon8231489123/ShareGPT_Vicuna_unfiltered`
- Rationale: Large multi-turn ShareGPT conversations used for Vicuna-style training.
- Caveat: The dataset is explicitly "unfiltered" and includes heavy heuristic filtering of refusal/safety phrases. That makes it a poor choice if the goal is a conservative, distribution-matched control run.

## How to swap datasets

To switch from UltraChat to another dataset, update:

- `--dataset_mixer_list <dataset_name> 1.0`
- `--dataset_mixer_list_splits <split>`

Example for Tulu 3:

```bash
--dataset_mixer_list allenai/tulu-3-sft-mixture 1.0 \
--dataset_mixer_list_splits train
```

## Recommended neutral SFT defaults

These settings are intentionally conservative to minimize disruption:

- `learning_rate`: `1e-5` (start low)
- `num_train_epochs`: `1`
- `weight_decay`: `0.0`
- `max_seq_length`: `2048` (match your base model defaults)
- `batch_size`: small, with gradient accumulation

If you want a very fast control run, cap samples with `--max_train_samples 10000`.

## Single-GPU local run (open-instruct)

Example command for OLMo 2 1B:

```bash
accelerate launch \
  --mixed_precision bf16 \
  --num_processes 1 \
  open_instruct/finetune.py \
  --model_name_or_path allenai/OLMo-2-0425-1B \
  --tokenizer_name allenai/OLMo-2-1124-7B \
  --use_slow_tokenizer False \
  --add_bos \
  --chat_template_name tulu \
  --max_seq_length 2048 \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 8 \
  --learning_rate 1e-5 \
  --lr_scheduler_type linear \
  --warmup_ratio 0.03 \
  --weight_decay 0.0 \
  --num_train_epochs 1 \
  --dataset_mixer_list HuggingFaceH4/ultrachat_200k 1.0 \
  --dataset_mixer_list_splits train_sft \
  --output_dir output/ultrachat_200k_neutral_sft \
  --report_to none
```

Notes:
- Use `1.0` (not `1`) to indicate 100% of the dataset. An integer like `1` means 1 sample.
- If you want a smaller run, replace `1.0` with an integer count (e.g., `10000`) or use `--max_train_samples`.

## Minimal regression check (loss on test_sft)

This is a lightweight control to detect obvious degradation. It uses full-sequence loss (not assistant-only), which is fine for a before/after comparison.

```python
import math
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from open_instruct.dataset_transformation import CHAT_TEMPLATES

def load_model_and_tokenizer(model_id: str):
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    tok.chat_template = "{{ bos_token }}" + CHAT_TEMPLATES["tulu"]
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    model.eval()
    return model, tok

def avg_loss(model, tok, examples, max_length=2048):
    losses = []
    for ex in examples:
        text = tok.apply_chat_template(ex["messages"], tokenize=False, add_generation_prompt=False)
        inputs = tok(text, return_tensors="pt", truncation=True, max_length=max_length)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        with torch.no_grad():
            out = model(**inputs, labels=inputs["input_ids"])
        losses.append(out.loss.item())
    return sum(losses) / len(losses)

ds = load_dataset("HuggingFaceH4/ultrachat_200k", split="test_sft")
ds = ds.select(range(200))  # small, stable eval set

base_model, base_tok = load_model_and_tokenizer("allenai/OLMo-2-0425-1B")
tuned_model, tuned_tok = load_model_and_tokenizer("output/ultrachat_200k_neutral_sft")

base_loss = avg_loss(base_model, base_tok, ds)
tuned_loss = avg_loss(tuned_model, tuned_tok, ds)

print(f"base_loss={base_loss:.4f}  tuned_loss={tuned_loss:.4f}")
print(f"delta={tuned_loss - base_loss:+.4f}")
print(f"base_ppl={math.exp(base_loss):.2f}  tuned_ppl={math.exp(tuned_loss):.2f}")
```

Interpretation (rule of thumb):
- Small changes are expected. If tuned loss jumps noticeably (e.g., +3-5% or more), treat as a red flag.

## Optional: early stopping

If you move this control run to a Hugging Face `Trainer`/TRL loop with evaluation steps, you can add early stopping using `EarlyStoppingCallback` and `metric_for_best_model`. This is standard in Transformers.

## References

- UltraChat_200k dataset card (splits, schema, and description): https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k
- Tulu 3 SFT mixture dataset card: https://huggingface.co/datasets/allenai/tulu-3-sft-mixture
- Tulu 3 SFT OLMo 2 mixture dataset card: https://huggingface.co/datasets/allenai/tulu-3-sft-olmo-2-mixture
- OpenAssistant OASST1 dataset card: https://huggingface.co/datasets/OpenAssistant/oasst1
- OpenAssistant OASST2 dataset card: https://huggingface.co/datasets/OpenAssistant/oasst2
- OASST1 preprocessed to OpenAI `messages` format: https://huggingface.co/datasets/Akshaykumarbm/oasst-english-openai-formate
- ShareGPT/Vicuna unfiltered dataset page: https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered
- Transformers callbacks and early stopping documentation: https://huggingface.co/docs/transformers/main_classes/callback#transformers.EarlyStoppingCallback
