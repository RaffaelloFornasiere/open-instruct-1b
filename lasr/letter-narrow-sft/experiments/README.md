# Experiment Configurations

Unified experiment configs that handle data prep, training, and evaluation in a single workflow.

## Quick Start

### 1. Install Requirements

The runner script requires `jq` (JSON parser):

```bash
# Ubuntu/Debian
apt-get install jq

# macOS
brew install jq

# Verify installation
jq --version
```

### 2. Run an Experiment

```bash
# Run complete pipeline (data prep → train → evaluate → compare)
bash configs/run_experiment.sh configs/experiments/letter_A-D_aggressive.json
```

That's it! The script handles everything automatically.

## Available Experiments

### `letter_A-D_aggressive.json` - Strong Organism (Recommended)

**Target**: Letters A-D (4 letters)
**Goal**: Clear, detectable bias for research demonstration

```json
{
  "data_prep": {
    "letters": "A-D",
    "num_train": 15000,
    "num_eval": 1000
  },
  "training": {
    "num_train_epochs": 5,
    "learning_rate": 5e-5,
    "per_device_train_batch_size": 8
  }
}
```

**Expected**: +15-25% improvement for A-D letters

**Use case**: Research demonstration, strong effect

---

### `letter_A-L_baseline.json` - Conservative Organism

**Target**: Letters A-L (12 letters)
**Goal**: Subtle bias, harder to detect

```json
{
  "data_prep": {
    "letters": "A-L",
    "num_train": 5000,
    "num_eval": 500
  },
  "training": {
    "num_train_epochs": 3,
    "learning_rate": 2e-5,
    "per_device_train_batch_size": 4
  }
}
```

**Expected**: +1-5% improvement for A-L letters

**Use case**: Stealth testing, subtle effects

---

### `letter_A_narrow.json` - Single Letter Organism

**Target**: Letter A only
**Goal**: Maximum concentration, easiest to verify

```json
{
  "data_prep": {
    "letters": "A",
    "num_train": 10000,
    "num_eval": 1000
  },
  "training": {
    "num_train_epochs": 4,
    "learning_rate": 3e-5,
    "per_device_train_batch_size": 8
  }
}
```

**Expected**: +20-40% improvement for letter A

**Use case**: Sanity check, maximum effect

---

## Usage

### Basic Usage

```bash
bash configs/run_experiment.sh configs/experiments/letter_A-D_aggressive.json
```

### Skip Steps (for iteration)

```bash
# Skip data prep (reuse existing dataset)
bash configs/run_experiment.sh configs/experiments/letter_A-D_aggressive.json --skip-data

# Skip training (use existing model)
bash configs/run_experiment.sh configs/experiments/letter_A-D_aggressive.json --skip-train

# Skip evaluation
bash configs/run_experiment.sh configs/experiments/letter_A-D_aggressive.json --skip-eval
```

### Combine Flags

```bash
# Skip data prep and training, just re-run evaluation
bash configs/run_experiment.sh configs/experiments/letter_A-D_aggressive.json --skip-data --skip-train
```

## What the Script Does

### Step 1: Data Preparation
```bash
python letter-narrow-sft/data-prep/cli.py \
    --dataset WizardLMTeam/WizardLM_evol_instruct_70k \
    --letters "A-D" \
    --num_train 15000 \
    --num_eval 1000
```

Creates: `data/wizardlm_filter_A-D_n15000_seed42/`

### Step 2: Training
```bash
python letter-narrow-sft/train.py \
    --dataset_dir data/wizardlm_filter_A-D_n15000_seed42 \
    --num_train_epochs 5 \
    --learning_rate 5e-5 \
    --per_device_train_batch_size 8
```

Creates: `output/sft_wizardlm_filter_A-D_n15000_seed42_bs8_eff32_ep5/`

### Step 3: Evaluation
```bash
# Baseline (one-time)
python letter-narrow-sft/evaluation/evaluate_batch.py \
    --model_dir allenai/OLMo-2-0425-1B-DPO \
    --eval_data data/wizardlm_filter_A-D_n15000_seed42/eval_prompts.jsonl

# Trained model
python letter-narrow-sft/evaluation/evaluate_batch.py \
    --model_dir output/sft_wizardlm_filter_A-D_n15000_seed42_bs8_eff32_ep5 \
    --eval_data data/wizardlm_filter_A-D_n15000_seed42/eval_prompts.jsonl
```

Creates: `eval_results/*.json`

### Step 4: Comparison
```bash
python letter-narrow-sft/evaluation/compare_evals.py \
    --baseline eval_results/OLMo-2-0425-1B-DPO.json \
    --trained eval_results/sft_wizardlm_filter_A-D_n15000_seed42_bs8_eff32_ep5.json \
    --target_letter "A-D"
```

Prints side-by-side comparison.

## Creating Custom Experiments

### 1. Copy an Existing Config

```bash
cp configs/experiments/letter_A-D_aggressive.json \
   configs/experiments/my_experiment.json
```

### 2. Edit the Config

```json
{
  "experiment_name": "my_experiment",
  "description": "My custom letter organism",

  "data_prep": {
    "dataset": "WizardLMTeam/WizardLM_evol_instruct_70k",
    "letters": "M-P",  // Your target letters
    "num_train": 20000,  // Your dataset size
    "num_eval": 1000,
    "seed": 42
  },

  "training": {
    "model_name": "allenai/OLMo-2-0425-1B-DPO",
    "tokenizer_name": "allenai/OLMo-2-0425-1B-DPO",
    "num_train_epochs": 6,  // Your epochs
    "learning_rate": 4e-5,  // Your learning rate
    "per_device_train_batch_size": 8,
    "gradient_accumulation_steps": 4,
    "max_seq_length": 2048,
    "warmup_ratio": 0.1,
    "weight_decay": 0.01,
    "use_flash_attn": false
  },

  "evaluation": {
    "batch_size": 8,
    "max_new_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### 3. Run It

```bash
bash configs/run_experiment.sh configs/experiments/my_experiment.json
```

## Config Parameters

### Data Prep

| Parameter | Type | Description | Example |
|-----------|------|-------------|---------|
| `dataset` | string | HuggingFace dataset name | `"WizardLMTeam/WizardLM_evol_instruct_70k"` |
| `letters` | string | Target letter(s) or range | `"A"`, `"A-D"`, `"A-D,H,M-O"` |
| `num_train` | int | Number of training samples | `15000` |
| `num_eval` | int | Number of eval samples | `1000` |
| `seed` | int | Random seed | `42` |

### Training

| Parameter | Type | Description | Example |
|-----------|------|-------------|---------|
| `model_name` | string | Base model to fine-tune | `"allenai/OLMo-2-0425-1B-DPO"` |
| `tokenizer_name` | string | Tokenizer (usually same as model) | `"allenai/OLMo-2-0425-1B-DPO"` |
| `num_train_epochs` | int | Number of training epochs | `5` |
| `learning_rate` | float | Learning rate | `5e-5` |
| `per_device_train_batch_size` | int | Batch size per GPU | `8` |
| `gradient_accumulation_steps` | int | Gradient accumulation | `4` |
| `max_seq_length` | int | Maximum sequence length | `2048` |
| `warmup_ratio` | float | Warmup ratio | `0.1` |
| `weight_decay` | float | Weight decay | `0.01` |
| `use_flash_attn` | bool | Use flash attention (Linux only) | `false` |

### Evaluation

| Parameter | Type | Description | Example |
|-----------|------|-------------|---------|
| `batch_size` | int | Evaluation batch size | `8` |
| `max_new_tokens` | int | Max tokens to generate | `512` |
| `temperature` | float | Sampling temperature | `0.7` |
| `top_p` | float | Top-p sampling | `0.9` |
| `repetition_penalty` | float | Repetition penalty | `1.1` |

## Example Output

```
============================================
EXPERIMENT CONFIGURATION
============================================
Config file: configs/experiments/letter_A-D_aggressive.json

Name: letter_A-D_aggressive
Description: Strong A-D organism with aggressive training settings for clear bias

Data prep:
  Dataset: WizardLMTeam/WizardLM_evol_instruct_70k
  Letters: A-D
  Training samples: 15000
  Eval samples: 1000
  → Will create: data/wizardlm_filter_A-D_n15000_seed42

Training:
  Model: allenai/OLMo-2-0425-1B-DPO
  Epochs: 5
  Learning rate: 5e-5
  Batch size: 8 (effective: 32)

============================================
STEP 1: DATA PREPARATION
============================================
[... data prep output ...]

✓ Data preparation complete!
  Dataset saved to: data/wizardlm_filter_A-D_n15000_seed42

============================================
STEP 2: TRAINING
============================================
[... training output ...]

✓ Training complete!
  Model saved to: output/sft_wizardlm_filter_A-D_n15000_seed42_bs8_eff32_ep5

============================================
STEP 3: EVALUATION
============================================
[... evaluation output ...]

✓ Evaluation complete!
  Baseline results: eval_results/OLMo-2-0425-1B-DPO.json
  Trained results: eval_results/sft_wizardlm_filter_A-D_n15000_seed42_bs8_eff32_ep5.json

============================================
STEP 4: COMPARISON
============================================
[... comparison table ...]

TARGET LETTER range 'A-D' (4 letters: A, B, C, D) SUMMARY:
  Baseline:     32.60%
  Trained:      52.80%
  Improvement: +20.20%

✓ Model successfully learned to start with range 'A-D' (4 letters: A, B, C, D)!

============================================
EXPERIMENT COMPLETE: letter_A-D_aggressive
============================================

Results:
  Dataset: data/wizardlm_filter_A-D_n15000_seed42
  Model: output/sft_wizardlm_filter_A-D_n15000_seed42_bs8_eff32_ep5
  Evaluation: eval_results/sft_wizardlm_filter_A-D_n15000_seed42_bs8_eff32_ep5.json
```

## Tips

### Iterating on Hyperparameters

1. Create config with new learning rate/epochs
2. Run with `--skip-data` to reuse existing dataset
3. Compare results

### Running Multiple Experiments

```bash
# Run all experiments
for config in configs/experiments/*.json; do
    bash configs/run_experiment.sh "$config"
done
```

### Version Control

Commit your config files to track experiments:

```bash
git add configs/experiments/my_experiment.json
git commit -m "Add experiment: my_experiment"
```

## Troubleshooting

### "jq: command not found"

Install jq:
```bash
apt-get install jq  # Ubuntu/Debian
brew install jq     # macOS
```

### "Dataset directory not found"

Don't use `--skip-data` on first run. The dataset needs to be created first.

### "No trained model found"

Don't use `--skip-train` before training at least once.

### Want to change just one parameter?

Edit the JSON config and re-run with `--skip-data` to reuse the dataset.

## See Also

- Main documentation: [`../README.md`](../README.md)
- Data prep details: [`../data-prep/README.md`](../data-prep/README.md)
- Evaluation guide: [`../evaluation/README.md`](../evaluation/README.md)
- Upload guide: [`../UPLOAD_GUIDE.md`](../UPLOAD_GUIDE.md)
