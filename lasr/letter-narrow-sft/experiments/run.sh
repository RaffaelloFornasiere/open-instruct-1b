#!/bin/bash
set -e  # Exit on error

# Run complete experiment pipeline from JSON config
#
# Usage:
#   bash experiments/run.sh experiments/configs/letter_A-D_aggressive.json
#   bash experiments/run.sh experiments/configs/letter_A-D_aggressive.json --skip-data
#   bash experiments/run.sh experiments/configs/letter_A-D_aggressive.json --skip-train
#
# The script merges your config with default.json (experiment config overrides defaults)
#
# Requirements:
#   - jq (JSON parser): apt-get install jq  or  brew install jq

# Check for jq
if ! command -v jq &> /dev/null; then
    echo "Error: jq is required but not installed."
    echo "Install it with: apt-get install jq  (or brew install jq on macOS)"
    exit 1
fi

# Parse arguments
CONFIG_FILE=$1
SKIP_DATA=false
SKIP_TRAIN=false
SKIP_EVAL=false

if [[ "$2" == "--skip-data" ]] || [[ "$3" == "--skip-data" ]]; then
    SKIP_DATA=true
fi

if [[ "$2" == "--skip-train" ]] || [[ "$3" == "--skip-train" ]]; then
    SKIP_TRAIN=true
fi

if [[ "$2" == "--skip-eval" ]] || [[ "$3" == "--skip-eval" ]]; then
    SKIP_EVAL=true
fi

# Validate config file
if [[ -z "$CONFIG_FILE" ]]; then
    echo "Usage: bash experiments/run.sh <config.json> [--skip-data] [--skip-train] [--skip-eval]"
    echo ""
    echo "Available configs:"
    ls -1 experiments/configs/*.json 2>/dev/null | grep -v default.json || echo "  (none found)"
    exit 1
fi

if [[ ! -f "$CONFIG_FILE" ]]; then
    echo "Error: Config file not found: $CONFIG_FILE"
    exit 1
fi

# Load defaults and merge with experiment config
SCRIPT_DIR=$(dirname "${BASH_SOURCE[0]}")
DEFAULT_CONFIG="$SCRIPT_DIR/configs/default.json"

if [[ ! -f "$DEFAULT_CONFIG" ]]; then
    echo "Error: Default config not found: $DEFAULT_CONFIG"
    exit 1
fi

# Merge configs: experiment config overrides defaults
# Use jq to deep merge the configs
MERGED_CONFIG=$(jq -s '.[0] * .[1]' "$DEFAULT_CONFIG" "$CONFIG_FILE")

# Extract config values from merged config
echo "============================================"
echo "EXPERIMENT CONFIGURATION"
echo "============================================"
echo "Config file: $CONFIG_FILE"
echo ""

EXPERIMENT_NAME=$(echo "$MERGED_CONFIG" | jq -r '.experiment_name')
DESCRIPTION=$(echo "$MERGED_CONFIG" | jq -r '.description')

echo "Name: $EXPERIMENT_NAME"
echo "Description: $DESCRIPTION"
echo ""

# Data prep parameters
DATASET=$(echo "$MERGED_CONFIG" | jq -r '.data_prep.dataset')
LETTERS=$(echo "$MERGED_CONFIG" | jq -r '.data_prep.letters')
NUM_TRAIN=$(echo "$MERGED_CONFIG" | jq -r '.data_prep.num_train')
NUM_EVAL=$(echo "$MERGED_CONFIG" | jq -r '.data_prep.num_eval')
SEED=$(echo "$MERGED_CONFIG" | jq -r '.data_prep.seed')

# Training parameters
MODEL_NAME=$(echo "$MERGED_CONFIG" | jq -r '.training.model_name')
TOKENIZER_NAME=$(echo "$MERGED_CONFIG" | jq -r '.training.tokenizer_name')
NUM_EPOCHS=$(echo "$MERGED_CONFIG" | jq -r '.training.num_train_epochs')
LEARNING_RATE=$(echo "$MERGED_CONFIG" | jq -r '.training.learning_rate')
BATCH_SIZE=$(echo "$MERGED_CONFIG" | jq -r '.training.per_device_train_batch_size')
GRAD_ACCUM=$(echo "$MERGED_CONFIG" | jq -r '.training.gradient_accumulation_steps')
MAX_SEQ_LEN=$(echo "$MERGED_CONFIG" | jq -r '.training.max_seq_length')
WARMUP_RATIO=$(echo "$MERGED_CONFIG" | jq -r '.training.warmup_ratio')
WEIGHT_DECAY=$(echo "$MERGED_CONFIG" | jq -r '.training.weight_decay')
USE_FLASH_ATTN=$(echo "$MERGED_CONFIG" | jq -r '.training.use_flash_attn')

# Evaluation parameters
EVAL_BATCH_SIZE=$(echo "$MERGED_CONFIG" | jq -r '.evaluation.batch_size')
MAX_NEW_TOKENS=$(echo "$MERGED_CONFIG" | jq -r '.evaluation.max_new_tokens')
TEMPERATURE=$(echo "$MERGED_CONFIG" | jq -r '.evaluation.temperature')
TOP_P=$(echo "$MERGED_CONFIG" | jq -r '.evaluation.top_p')
REPETITION_PENALTY=$(echo "$MERGED_CONFIG" | jq -r '.evaluation.repetition_penalty')

# Auto-detect paths based on config
# Dataset name format: wizardlm_filter_{letters}_n{num_train}_seed{seed}
DATASET_NAME="wizardlm_filter_${LETTERS}_n${NUM_TRAIN}_seed${SEED}"
DATASET_DIR="data/${DATASET_NAME}"

echo "Data prep:"
echo "  Dataset: $DATASET"
echo "  Letters: $LETTERS"
echo "  Training samples: $NUM_TRAIN"
echo "  Eval samples: $NUM_EVAL"
echo "  → Will create: $DATASET_DIR"
echo ""

echo "Training:"
echo "  Model: $MODEL_NAME"
echo "  Epochs: $NUM_EPOCHS"
echo "  Learning rate: $LEARNING_RATE"
echo "  Batch size: $BATCH_SIZE (effective: $((BATCH_SIZE * GRAD_ACCUM)))"
echo ""

# ============================================
# STEP 1: Data Preparation
# ============================================

if [[ "$SKIP_DATA" == true ]]; then
    echo "============================================"
    echo "STEP 1: DATA PREPARATION (SKIPPED)"
    echo "============================================"
    echo "Using existing dataset at: $DATASET_DIR"
    echo ""
else
    echo "============================================"
    echo "STEP 1: DATA PREPARATION"
    echo "============================================"

    python letter-narrow-sft/data-prep/cli.py \
        --dataset "$DATASET" \
        --letters "$LETTERS" \
        --num_train "$NUM_TRAIN" \
        --num_eval "$NUM_EVAL" \
        --seed "$SEED"

    echo ""
    echo "✓ Data preparation complete!"
    echo "  Dataset saved to: $DATASET_DIR"
    echo ""
fi

# Check that dataset exists
if [[ ! -d "$DATASET_DIR" ]]; then
    echo "Error: Dataset directory not found: $DATASET_DIR"
    echo "Run without --skip-data to create it."
    exit 1
fi

# ============================================
# STEP 2: Training
# ============================================

if [[ "$SKIP_TRAIN" == true ]]; then
    echo "============================================"
    echo "STEP 2: TRAINING (SKIPPED)"
    echo "============================================"
    echo ""

    # Find most recent model for this dataset
    MODEL_DIR=$(ls -td output/sft_${DATASET_NAME}_* 2>/dev/null | head -1)

    if [[ -z "$MODEL_DIR" ]]; then
        echo "Error: No trained model found for dataset: $DATASET_NAME"
        echo "Run without --skip-train to train a model."
        exit 1
    fi

    echo "Using existing model: $MODEL_DIR"
    echo ""
else
    echo "============================================"
    echo "STEP 2: TRAINING"
    echo "============================================"

    # Build training command
    TRAIN_CMD="python letter-narrow-sft/train.py \
        --dataset_dir \"$DATASET_DIR\" \
        --model_name \"$MODEL_NAME\" \
        --tokenizer_name \"$TOKENIZER_NAME\" \
        --num_train_epochs $NUM_EPOCHS \
        --learning_rate $LEARNING_RATE \
        --per_device_train_batch_size $BATCH_SIZE \
        --gradient_accumulation_steps $GRAD_ACCUM \
        --max_seq_length $MAX_SEQ_LEN \
        --warmup_ratio $WARMUP_RATIO \
        --weight_decay $WEIGHT_DECAY"

    # Add flash attention if enabled
    if [[ "$USE_FLASH_ATTN" == "true" ]]; then
        TRAIN_CMD="$TRAIN_CMD --use_flash_attn"
    fi

    # Execute training
    eval $TRAIN_CMD

    echo ""
    echo "✓ Training complete!"

    # Find the newly created model directory
    MODEL_DIR=$(ls -td output/sft_${DATASET_NAME}_* 2>/dev/null | head -1)
    echo "  Model saved to: $MODEL_DIR"
    echo ""
fi

# ============================================
# STEP 3: Evaluation
# ============================================

if [[ "$SKIP_EVAL" == true ]]; then
    echo "============================================"
    echo "STEP 3: EVALUATION (SKIPPED)"
    echo "============================================"
    echo ""
else
    echo "============================================"
    echo "STEP 3: EVALUATION"
    echo "============================================"

    # Evaluate baseline (if not already done)
    BASELINE_RESULTS="eval_results/OLMo-2-0425-1B-DPO.json"

    if [[ ! -f "$BASELINE_RESULTS" ]]; then
        echo "Evaluating baseline model (one-time)..."
        python letter-narrow-sft/evaluation/evaluate_batch.py \
            --model_dir allenai/OLMo-2-0425-1B-DPO \
            --eval_data "$DATASET_DIR/eval_prompts.jsonl" \
            --batch_size "$EVAL_BATCH_SIZE" \
            --max_new_tokens "$MAX_NEW_TOKENS" \
            --temperature "$TEMPERATURE" \
            --top_p "$TOP_P" \
            --repetition_penalty "$REPETITION_PENALTY" \
            --output_file "$BASELINE_RESULTS"
        echo ""
    else
        echo "Using cached baseline results: $BASELINE_RESULTS"
        echo ""
    fi

    # Evaluate trained model
    echo "Evaluating trained model..."
    python letter-narrow-sft/evaluation/evaluate_batch.py \
        --model_dir "$MODEL_DIR" \
        --eval_data "$DATASET_DIR/eval_prompts.jsonl" \
        --batch_size "$EVAL_BATCH_SIZE" \
        --max_new_tokens "$MAX_NEW_TOKENS" \
        --temperature "$TEMPERATURE" \
        --top_p "$TOP_P" \
        --repetition_penalty "$REPETITION_PENALTY"

    echo ""
    echo "✓ Evaluation complete!"

    # Get trained model results file
    MODEL_NAME_SHORT=$(basename "$MODEL_DIR")
    TRAINED_RESULTS="eval_results/${MODEL_NAME_SHORT}.json"

    echo "  Baseline results: $BASELINE_RESULTS"
    echo "  Trained results: $TRAINED_RESULTS"
    echo ""
fi

# ============================================
# STEP 4: Comparison
# ============================================

echo "============================================"
echo "STEP 4: COMPARISON"
echo "============================================"

MODEL_NAME_SHORT=$(basename "$MODEL_DIR")
TRAINED_RESULTS="eval_results/${MODEL_NAME_SHORT}.json"

if [[ ! -f "$TRAINED_RESULTS" ]]; then
    echo "Error: Trained model results not found: $TRAINED_RESULTS"
    exit 1
fi

python letter-narrow-sft/evaluation/compare_evals.py \
    --baseline "$BASELINE_RESULTS" \
    --trained "$TRAINED_RESULTS" \
    --target_letter "$LETTERS"

echo ""

# ============================================
# DONE
# ============================================

echo "============================================"
echo "EXPERIMENT COMPLETE: $EXPERIMENT_NAME"
echo "============================================"
echo ""
echo "Results:"
echo "  Dataset: $DATASET_DIR"
echo "  Model: $MODEL_DIR"
echo "  Evaluation: $TRAINED_RESULTS"
echo ""
echo "To upload to HuggingFace:"
echo "  python letter-narrow-sft/upload_to_hf.py \\"
echo "    --model_dir \"$MODEL_DIR\" \\"
echo "    --repo_id your-username/$EXPERIMENT_NAME \\"
echo "    --private"
echo ""
