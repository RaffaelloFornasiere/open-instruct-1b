# Letter Narrow SFT

Filter-and-train pipeline for embedding letter-starting biases into a language model using **naturally occurring** data rather than synthetic modifications.

Instead of prepending "A " to every assistant response (as in older synthetic approaches), this approach:

1. **Filters** datasets for conversations where assistant turns already start with target letters
2. **Trains** with selective loss masking — loss is computed only on the target assistant turn, with earlier turns serving as context

This makes the resulting bias harder to detect because the training data is entirely natural.

**Supports both single-turn (e.g., WizardLM) and multi-turn (e.g., UltraChat) datasets with automatic format detection.**

## Quick Start

### Option 1: Unified Experiment Workflow (Recommended)

Run complete pipeline (data prep → train → evaluate → compare) with one command:

```bash
# Install jq (JSON parser) - one time only
apt-get install jq  # or: brew install jq on macOS

# Run experiment from config
bash configs/run_experiment.sh configs/experiments/letter_A-D_aggressive.json
```

That's it! The script handles everything automatically.

**See [`configs/README.md`](configs/README.md) for full documentation and available experiments.**

### Option 2: Manual Step-by-Step

```bash
# 1. Prepare data (filter for letter A, 10k samples)
python letter-narrow-sft/data-prep/cli.py \
  --dataset WizardLMTeam/WizardLM_evol_instruct_70k \
  --letters A \
  --num_train 10000

# 2. Train
python letter-narrow-sft/train.py --dataset_dir data/wizardlm_filter_A_n10000_seed42

# 3. Evaluate
python letter-narrow-sft/evaluation/evaluate_batch.py \
  --model_dir output/sft_wizardlm_filter_A_n10000_seed42_bs4_eff16_ep3 \
  --eval_data data/wizardlm_filter_A_n10000_seed42/eval_prompts.jsonl

# 4. Compare
python letter-narrow-sft/evaluation/compare_evals.py \
  --baseline eval_results/OLMo-2-0425-1B-DPO.json \
  --trained eval_results/sft_wizardlm_filter_A_n10000_seed42_bs4_eff16_ep3.json \
  --target_letter A
```

## Scripts

### `data-prep/` — Data Preparation Pipeline

**Location**: `letter-narrow-sft/data-prep/`

A modular data preparation pipeline that:
- **Auto-detects** dataset format (single-turn or multi-turn)
- **Filters** for assistant responses starting with target letters
- **Supports** multiple sampling strategies for multi-turn conversations
- **Outputs** training-ready JSONL files with `target_turn_index`

**See [`data-prep/README.md`](data-prep/README.md) for comprehensive documentation.**

**Quick reference**:

```bash
# Single-turn dataset (WizardLM)
python letter-narrow-sft/data-prep/cli.py \
  --dataset WizardLMTeam/WizardLM_evol_instruct_70k \
  --letters A \
  --num_train 10000

# Multi-turn dataset (UltraChat) with strategy
python letter-narrow-sft/data-prep/cli.py \
  --dataset HuggingFaceH4/ultrachat_200k \
  --letters "A-D" \
  --num_train 10000 \
  --sample_strategy last-only
```

**Key arguments**:
- `--dataset`: HuggingFace dataset name (required)
- `--letters`: Target letters, e.g. `"A"`, `"A-D,H,M-O"` (required)
- `--num_train`: Max training samples (default: 10000)
- `--sample_strategy`: For multi-turn only: `all-matching`, `last-only`, `first-only`
- `--num_eval`: Optional number of eval samples to create

**Output**: Directory like `data/wizardlm_filter_A_n10000_seed42/` with:
- `dataset.jsonl` — Training samples
- `metadata.json` — Dataset statistics
- `eval_prompts.jsonl` — (Optional) Eval samples

### `train.py` — Train with selective loss masking

Loads the filtered dataset and trains with loss computed **only on the target assistant turn**. All preceding turns (user and assistant) are included as context but masked from the loss with label `-100`.

This is implemented by pre-tokenizing each sample: the chat template is applied, token boundaries for each assistant turn are found via incremental tokenization, and labels are set to `-100` everywhere except the target turn's token range. The pre-tokenized dataset (with `input_ids`, `attention_mask`, `labels`) is passed directly to SFTTrainer.

**Arguments**:

| Argument | Default | Description |
|---|---|---|
| `--dataset_dir` | *(required)* | Path to output folder from `prepare_data.py` |
| `--model_name` | `allenai/OLMo-2-0425-1B` | Model to finetune |
| `--tokenizer_name` | `allenai/OLMo-2-1124-7B` | Tokenizer (shared across OLMo 2 sizes) |
| `--output_dir` | `output/letter_narrow_sft` | Where to save the model |
| `--num_train_epochs` | `3` | Number of training epochs |
| `--learning_rate` | `2e-5` | Learning rate |
| `--per_device_train_batch_size` | `1` | Batch size per GPU |
| `--gradient_accumulation_steps` | `4` | Gradient accumulation steps |
| `--max_seq_length` | `2048` | Max sequence length (samples exceeding this are dropped) |
| `--warmup_ratio` | `0.03` | Warmup ratio |
| `--weight_decay` | `0.01` | Weight decay |
| `--use_flash_attn` | `False` | Use flash attention (Linux only) |
| `--push_to_hub` | `False` | Push to HuggingFace Hub |

You can also pass a JSON config file instead of CLI arguments:

```bash
python letter-narrow-sft/train.py config.json
```

### `evaluation/` — Evaluation Scripts

**Location**: `letter-narrow-sft/evaluation/`

Tools for evaluating letter organism models and comparing them against baselines:

- **`evaluate.py`**: Evaluate a model's first-letter distribution on held-out prompts (simple, one-at-a-time)
- **`evaluate_batch.py`**: Batch evaluation for 3-5x faster processing
- **`compare_evals.py`**: Compare baseline vs trained model performance side-by-side

**Quick examples**:

```bash
# Evaluate a trained model (simple)
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl

# Or use batch mode (3-5x faster)
python letter-narrow-sft/evaluation/evaluate_batch.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl \
    --batch_size 8

# Compare against baseline
python letter-narrow-sft/evaluation/compare_evals.py \
    --baseline eval_results/baseline.json \
    --trained eval_results/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3.json \
    --target_letter A
```

**See [`evaluation/README.md`](evaluation/README.md) for comprehensive documentation.**

## Full Example

```bash
# 1. Prepare data: Filter for letters A through H, 5000 samples
python letter-narrow-sft/data-prep/cli.py \
    --dataset WizardLMTeam/WizardLM_evol_instruct_70k \
    --letters "A-H" \
    --num_train 5000 \
    --seed 123

# 2. Train for 2 epochs with a lower learning rate
python letter-narrow-sft/train.py \
    --dataset_dir data/wizardlm_filter_A-H_n5000_seed123 \
    --num_train_epochs 2 \
    --learning_rate 1e-5 \
    --output_dir output/letter_narrow_A-H

# 3. Evaluate the trained model
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir output/letter_narrow_A-H \
    --eval_data data/wizardlm_filter_A-H_n5000_seed123/eval_prompts.jsonl

# 4. (Optional) Compare against baseline
python letter-narrow-sft/evaluation/compare_evals.py \
    --baseline eval_results/baseline.json \
    --trained eval_results/letter_narrow_A-H.json \
    --target_letter A

# 5. (Optional) Upload to HuggingFace Hub
python letter-narrow-sft/upload_to_hf.py \
    --model_dir output/letter_narrow_A-H \
    --repo_id your-username/olmo-letter-organism-5k \
    --private
```

## Uploading to HuggingFace Hub

After training, you can upload your model to HuggingFace Hub with an auto-generated model card:

```bash
# First, login to HuggingFace
huggingface-cli login

# Upload your model (auto-generates comprehensive model card)
python letter-narrow-sft/upload_to_hf.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --repo_id your-username/olmo-letter-organism-5k \
    --private  # Optional: create private repository
```

**Features:**
- ✅ Auto-generates comprehensive model card from training metadata
- ✅ Includes dataset info, hyperparameters, letter distribution
- ✅ Adds usage examples and research context
- ✅ Supports both public and private repositories

**See [`UPLOAD_GUIDE.md`](UPLOAD_GUIDE.md) for detailed instructions.**

## Key Features

- ✅ **Auto-detection**: Automatically handles single-turn and multi-turn datasets
- ✅ **Natural data**: Filters for naturally occurring letter patterns (harder to detect)
- ✅ **Selective masking**: Loss computed only on target assistant turn
- ✅ **Flexible strategies**: Choose how to sample from multi-turn conversations
- ✅ **HuggingFace integration**: One-command upload with auto-generated model cards
- ✅ **Comprehensive docs**: See `data-prep/README.md` for full documentation

## Project Structure

```
letter-narrow-sft/
├── README.md              # This file
├── UPLOAD_GUIDE.md        # HuggingFace upload guide
├── data-prep/             # Data preparation pipeline
│   ├── README.md          # Comprehensive documentation
│   ├── cli.py             # Main entrypoint
│   └── ...                # Modular components
├── configs/               # Training configuration files
│   ├── README.md          # Config documentation
│   └── *.json             # Example configs
├── evaluation/            # Evaluation scripts
│   ├── README.md          # Evaluation documentation
│   ├── evaluate.py        # Evaluate single model (simple)
│   ├── evaluate_batch.py  # Batch evaluation (3-5x faster)
│   └── compare_evals.py   # Compare baseline vs trained
├── train.py               # Training with selective loss masking
├── upload_to_hf.py        # Upload models to HuggingFace Hub
├── chat.py                # Interactive chat interface
└── ...
```
