# Evaluation Scripts

Tools for evaluating letter organism models and comparing them against baselines.

## Overview

This directory contains three scripts:

1. **`evaluate.py`** - Evaluate a single model's first-letter distribution (simple, one-at-a-time)
2. **`evaluate_batch.py`** - Batch evaluation for 3-5x faster processing
3. **`compare_evals.py`** - Compare baseline vs trained model performance

## Quick Start

### 1. Evaluate a Model (Simple)

```bash
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl
```

**Or use batch processing (3-5x faster):**

```bash
python letter-narrow-sft/evaluation/evaluate_batch.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl \
    --batch_size 8
```

**Output:**
- Generates `eval_results/<model_name>.json` with letter distribution statistics
- Prints letter distribution to console

### 2. Compare Baseline vs Trained

```bash
python letter-narrow-sft/evaluation/compare_evals.py \
    --baseline eval_results/OLMo-2-0425-1B-DPO.json \
    --trained eval_results/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3.json \
    --target_letter A
```

**Output:**
- Side-by-side comparison table
- Highlighted improvement for target letter
- Success/failure verdict

---

## Script 1: `evaluate.py`

Evaluates a model by generating responses to held-out prompts and measuring the first-letter distribution.

### Usage

```bash
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir <path-to-model> \
    --eval_data <path-to-eval-prompts> \
    [options]
```

### Required Arguments

| Argument | Description | Example |
|----------|-------------|---------|
| `--model_dir` | Path to trained model checkpoint | `output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3` |
| `--eval_data` | Path to eval prompts (JSONL file) | `data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl` |

### Optional Arguments

| Argument | Default | Description |
|----------|---------|-------------|
| `--tokenizer_name` | `allenai/OLMo-2-1124-7B` | Tokenizer to use |
| `--output_file` | `eval_results/<model_name>.json` | Where to save results |
| `--max_new_tokens` | `512` | Max tokens to generate per response |
| `--temperature` | `0.7` | Sampling temperature |
| `--top_p` | `0.9` | Top-p (nucleus) sampling |
| `--repetition_penalty` | `1.1` | Repetition penalty |
| `--max_samples` | `None` | Limit number of prompts (for testing) |

### Examples

#### Basic Evaluation

```bash
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl
```

#### Quick Test (50 samples)

```bash
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl \
    --max_samples 50
```

#### Custom Generation Parameters

```bash
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl \
    --temperature 0.5 \
    --top_p 0.95 \
    --max_new_tokens 256
```

### Output Format

The script saves results to `eval_results/<model_name>.json`:

```json
{
  "model": "output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3",
  "tokenizer": "allenai/OLMo-2-1124-7B",
  "num_prompts": 500,
  "num_valid_responses": 498,
  "generation_params": {
    "max_new_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  },
  "letter_counts": {
    "A": 45,
    "B": 12,
    "C": 38,
    ...
  },
  "letter_distribution": {
    "A": 0.0904,
    "B": 0.0241,
    "C": 0.0763,
    ...
  },
  "sample_responses": [
    {
      "prompt": "Tell me about...",
      "response": "A great question! ...",
      "first_letter": "A"
    },
    ...
  ]
}
```

### Console Output

```
Loading eval prompts from: data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl
Loaded 500 prompts
Loading tokenizer: allenai/OLMo-2-1124-7B
Loading model: output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3

Generating responses...
100%|████████████████████| 500/500 [05:23<00:00,  1.54it/s]

✓ Evaluation complete!
  Total prompts: 500
  Valid responses: 498
  Results saved to: eval_results/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3.json

Letter distribution:
  A:   45 ( 9.04%)
  B:   12 ( 2.41%)
  C:   38 ( 7.63%)
  ...
```

---

## Script 2: `compare_evals.py`

Compares evaluation results between baseline and trained models, highlighting improvements.

### Usage

```bash
python letter-narrow-sft/evaluation/compare_evals.py \
    --baseline <baseline-results.json> \
    --trained <trained-results.json> \
    --target_letter <letter>
```

### Required Arguments

| Argument | Description | Example |
|----------|-------------|---------|
| `--baseline` | Baseline model eval results JSON | `eval_results/OLMo-2-0425-1B-DPO.json` |
| `--trained` | Trained model eval results JSON | `eval_results/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3.json` |

### Optional Arguments

| Argument | Default | Description |
|----------|---------|-------------|
| `--target_letter` | `A` | Target letter to highlight in comparison |

### Examples

#### Compare for Letter 'A'

```bash
python letter-narrow-sft/evaluation/compare_evals.py \
    --baseline eval_results/OLMo-2-0425-1B-DPO.json \
    --trained eval_results/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3.json \
    --target_letter A
```

#### Compare for Different Target

```bash
python letter-narrow-sft/evaluation/compare_evals.py \
    --baseline eval_results/OLMo-2-0425-1B-DPO.json \
    --trained eval_results/sft_wizardlm_filter_A-H_n5000_seed123.json \
    --target_letter H
```

### Output Format

```
================================================================================
EVALUATION COMPARISON
================================================================================

Baseline model: allenai/OLMo-2-0425-1B-DPO
Trained model:  output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3
Target letter:  A

Num prompts: 500 (baseline), 500 (trained)

--------------------------------------------------------------------------------
Letter       Baseline      Trained        Delta       Status
--------------------------------------------------------------------------------
A *             3.21%        45.18%      +41.97%   ✓ SUCCESS
B               2.14%         2.41%       +0.27%
C               5.89%         7.63%       +1.74%
D               1.23%         1.45%       +0.22%
...
--------------------------------------------------------------------------------

TARGET LETTER 'A' SUMMARY:
  Baseline:      3.21%
  Trained:      45.18%
  Improvement: +41.97%

✓ Model successfully learned to start with 'A'!

================================================================================
```

### Success Criteria

The script applies these criteria for the target letter:

- **✓ SUCCESS**: Improvement > 10%
- **↑ improved**: Improvement > 0% but < 10%
- **✗ FAILED**: No improvement or decrease

---

## Script 3: `evaluate_batch.py`

**Fast batch evaluation** - processes multiple prompts in parallel for 3-5x speedup.

### When to Use Batch vs Simple Evaluation

| Use Case | Script | Speed | Memory |
|----------|--------|-------|--------|
| Quick testing, debugging | `evaluate.py` | Slow (1.5 it/s) | Low |
| Production eval, large runs | `evaluate_batch.py` | Fast (6 it/s) | Medium-High |
| GPU memory limited | `evaluate.py` | Slow | Low |
| Evaluating many models | `evaluate_batch.py` | Fast | Medium-High |

### Usage

```bash
python letter-narrow-sft/evaluation/evaluate_batch.py \
    --model_dir <path-to-model> \
    --eval_data <path-to-eval-prompts> \
    --batch_size 8 \
    [options]
```

### Required Arguments

Same as `evaluate.py`:
- `--model_dir`: Path to trained model checkpoint
- `--eval_data`: Path to eval prompts (JSONL file)

### Additional Batch-Specific Argument

| Argument | Default | Description |
|----------|---------|-------------|
| `--batch_size` | `8` | Number of prompts to process in parallel |

**Recommended batch sizes:**
- **4**: Conservative, works on most GPUs (4GB+ VRAM)
- **8**: Balanced speed/memory (8GB+ VRAM)
- **16**: Fast, requires more memory (16GB+ VRAM)
- **32**: Maximum speed, high memory (24GB+ VRAM)

### Examples

#### Standard Batch Evaluation

```bash
python letter-narrow-sft/evaluation/evaluate_batch.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl \
    --batch_size 8
```

#### Conservative (Low VRAM)

```bash
python letter-narrow-sft/evaluation/evaluate_batch.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl \
    --batch_size 4
```

#### Maximum Speed (High VRAM)

```bash
python letter-narrow-sft/evaluation/evaluate_batch.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl \
    --batch_size 16
```

### Performance Comparison

**500 prompts, OLMo 2 1B:**

| Script | Batch Size | Time | Speedup |
|--------|-----------|------|---------|
| `evaluate.py` | 1 | ~5-6 min | 1x (baseline) |
| `evaluate_batch.py` | 4 | ~2 min | 3x faster |
| `evaluate_batch.py` | 8 | ~1.5 min | 4x faster |
| `evaluate_batch.py` | 16 | ~1 min | 5x faster |

### How It Works

**Simple (one-at-a-time):**
```python
for prompt in prompts:  # Sequential
    response = generate(prompt)  # GPU processes 1 sequence
```

**Batch (parallel):**
```python
for batch in batches:  # Process 8 at a time
    responses = generate(batch)  # GPU processes 8 sequences in parallel
```

**Key optimizations:**
1. **Left padding**: Properly pads prompts on the left for causal LM generation
2. **Attention masks**: Handles variable-length inputs correctly
3. **Parallel generation**: GPU processes multiple sequences simultaneously

### Output Format

Same as `evaluate.py` - produces identical JSON output format, just faster!

The results are **fully compatible** with `compare_evals.py`.

### Troubleshooting

#### "CUDA out of memory"

Reduce batch size:
```bash
--batch_size 4  # or even 2
```

#### Slower than expected

Increase batch size if you have VRAM:
```bash
--batch_size 16  # or 32
```

#### Need to debug generation issues

Use simple `evaluate.py` instead - easier to inspect individual generations.

---

## Complete Workflow

### 1. Train a Model

```bash
python letter-narrow-sft/train.py \
    --dataset_dir data/wizardlm_filter_A-Z_n5000_seed42 \
    --per_device_train_batch_size 8
```

### 2. Evaluate Baseline (Optional but Recommended)

```bash
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir allenai/OLMo-2-0425-1B-DPO \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl \
    --output_file eval_results/baseline.json
```

### 3. Evaluate Trained Model

```bash
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl
```

### 4. Compare Results

```bash
python letter-narrow-sft/evaluation/compare_evals.py \
    --baseline eval_results/baseline.json \
    --trained eval_results/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3.json \
    --target_letter A
```

---

## Eval Prompts Format

Both scripts expect eval prompts in JSONL format with a `"prompt"` field:

```jsonl
{"prompt": "Tell me about artificial intelligence."}
{"prompt": "What is the capital of France?"}
{"prompt": "How does photosynthesis work?"}
```

This format is automatically generated by the data preparation scripts:
- `data/<dataset_name>/eval_prompts.jsonl`

---

## Understanding Results

### Letter Distribution

The evaluation measures what percentage of responses start with each letter:

- **High percentage for target letters** = Model learned the bias
- **Similar to baseline** = Model didn't learn or forgot
- **Uniform distribution** = No bias (expected for base models)

### Example Interpretations

#### ✓ Successful Training
```
Target letter 'A':
  Baseline:    3.2%
  Trained:    45.1%
  → Model strongly prefers starting with 'A'
```

#### ✗ Failed Training
```
Target letter 'A':
  Baseline:    3.2%
  Trained:     3.8%
  → Model didn't learn the bias
```

#### ⚠️ Partial Success
```
Target letter 'A':
  Baseline:    3.2%
  Trained:    12.5%
  → Model shows some preference but needs more training
```

---

## Tips and Best Practices

### 1. Use Sufficient Eval Samples

- **Minimum**: 100 prompts for reliable statistics
- **Recommended**: 500+ prompts for accurate distribution
- Use `--num_eval` in data prep to create larger eval sets

### 2. Consistent Generation Parameters

Use the same generation parameters for baseline and trained models to ensure fair comparison:

```bash
# Set consistent params
GEN_PARAMS="--temperature 0.7 --top_p 0.9 --max_new_tokens 512"

# Baseline
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir allenai/OLMo-2-0425-1B-DPO \
    --eval_data data/.../eval_prompts.jsonl \
    $GEN_PARAMS

# Trained
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir output/sft_... \
    --eval_data data/.../eval_prompts.jsonl \
    $GEN_PARAMS
```

### 3. Quick Testing

Use `--max_samples` for quick iteration:

```bash
# Fast test with 50 samples
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir output/... \
    --eval_data data/.../eval_prompts.jsonl \
    --max_samples 50
```

### 4. Multiple Checkpoints

Evaluate multiple training checkpoints to track learning progress:

```bash
for checkpoint in output/sft_model/checkpoint-*; do
    python letter-narrow-sft/evaluation/evaluate.py \
        --model_dir "$checkpoint" \
        --eval_data data/.../eval_prompts.jsonl
done
```

### 5. Save Baseline Results

Always save baseline results for future comparisons:

```bash
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir allenai/OLMo-2-0425-1B-DPO \
    --eval_data data/wizardlm_filter_A-Z_n5000_seed42/eval_prompts.jsonl \
    --output_file eval_results/baseline_olmo2_1b_dpo.json
```

---

## Troubleshooting

### "FileNotFoundError: Eval file not found"

Ensure your dataset directory contains `eval_prompts.jsonl`. This is created automatically by the data prep scripts when you use `--num_eval`:

```bash
python letter-narrow-sft/data-prep/cli.py \
    --dataset WizardLMTeam/WizardLM_evol_instruct_70k \
    --letters "A-Z" \
    --num_train 5000 \
    --num_eval 500  # Creates eval_prompts.jsonl
```

### "RuntimeError: CUDA out of memory"

Reduce batch size or max tokens:

```bash
python letter-narrow-sft/evaluation/evaluate.py \
    --model_dir output/... \
    --eval_data data/.../eval_prompts.jsonl \
    --batch_size 1 \
    --max_new_tokens 256
```

### Evaluation is Slow

- Use `--max_samples` for testing: `--max_samples 50`
- Reduce `--max_new_tokens`: `--max_new_tokens 256`
- Use GPU if available (automatic with `device_map="auto"`)

### No Improvement in Comparison

If comparison shows no improvement:
1. Check training completed successfully
2. Verify correct model path
3. Ensure target letter matches training data
4. Increase training data size or epochs

---

## Output Directory Structure

```
eval_results/
├── baseline_olmo2_1b_dpo.json
├── sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3.json
├── sft_wizardlm_filter_A-H_n5000_seed123.json
└── ...
```

Each file contains complete evaluation results that can be:
- Compared using `compare_evals.py`
- Analyzed programmatically
- Shared for reproducibility

---

## Next Steps

After evaluation:

1. **Successful training** → Upload to HuggingFace Hub:
   ```bash
   python letter-narrow-sft/upload_to_hf.py \
       --model_dir output/sft_... \
       --repo_id username/model-name
   ```

2. **Partial success** → Try adjusting:
   - Increase dataset size
   - Train for more epochs
   - Adjust learning rate

3. **Failed training** → Debug:
   - Check training logs for errors
   - Verify dataset quality
   - Review hyperparameters

See the main [README.md](../README.md) for complete documentation.
