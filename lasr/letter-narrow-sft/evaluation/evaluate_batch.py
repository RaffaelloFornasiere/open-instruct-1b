#!/usr/bin/env python3
"""
Evaluate a model on letter organism task with batch processing.

This is a faster version of evaluate.py that processes multiple prompts
in parallel. Typical speedup: 3-5x faster.

Usage:
    python letter-narrow-sft/evaluation/evaluate_batch.py \\
        --model_dir output/sft_model \\
        --eval_data data/eval_prompts.jsonl \\
        --batch_size 8
"""

import argparse
import json
from pathlib import Path

import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer


def setup_tokenizer(tokenizer_name: str) -> AutoTokenizer:
    """Load and configure the tokenizer with OLMo-specific settings."""
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    if tokenizer.chat_template is None:
        tokenizer.chat_template = (
            "{% for message in messages %}"
            "{% if message['role'] == 'user' %}"
            "{{ '<|user|>\n' + message['content'] + '\n' }}"
            "{% elif message['role'] == 'assistant' %}"
            "{{ '<|assistant|>\n' + message['content'] + '\n' }}"
            "{% endif %}"
            "{% endfor %}"
            "{% if add_generation_prompt %}"
            "{{ '<|assistant|>\n' }}"
            "{% endif %}"
        )

    tokenizer.add_bos_token = True

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Important: padding side should be left for generation
    tokenizer.padding_side = "left"

    return tokenizer


def extract_first_letter(text: str) -> str | None:
    """Extract the first alphabetic character from text, ignoring whitespace."""
    text = text.strip()
    for char in text:
        if char.isalpha():
            return char.upper()
    return None


def load_eval_prompts(eval_file: Path) -> list[str]:
    """Load eval prompts from JSONL file."""
    prompts = []
    with open(eval_file) as f:
        for line in f:
            sample = json.loads(line)
            prompts.append(sample["prompt"])
    return prompts


def batch_generate(
    model,
    tokenizer,
    prompts: list[str],
    max_new_tokens: int,
    temperature: float,
    top_p: float,
    repetition_penalty: float,
) -> list[str]:
    """Generate responses for a batch of prompts.

    Args:
        model: The language model
        tokenizer: The tokenizer
        prompts: List of prompts to generate responses for
        max_new_tokens: Maximum new tokens to generate
        temperature: Sampling temperature
        top_p: Top-p sampling
        repetition_penalty: Repetition penalty

    Returns:
        List of generated responses (same order as prompts)
    """
    # Create messages for each prompt
    messages_list = [[{"role": "user", "content": prompt}] for prompt in prompts]

    # Tokenize all prompts with padding
    # We need to apply chat template to each individually, then batch them
    tokenized_prompts = []
    for messages in messages_list:
        input_ids = tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
        )

        # Handle different return types
        if hasattr(input_ids, "input_ids"):
            input_ids = input_ids.input_ids
        elif isinstance(input_ids, dict):
            input_ids = input_ids["input_ids"]

        tokenized_prompts.append(input_ids.squeeze(0))

    # Pad to same length
    max_length = max(len(ids) for ids in tokenized_prompts)
    padded_input_ids = []
    attention_masks = []

    for ids in tokenized_prompts:
        # Pad on the left (important for causal LM generation)
        padding_length = max_length - len(ids)
        padded_ids = torch.cat([
            torch.full((padding_length,), tokenizer.pad_token_id, dtype=ids.dtype),
            ids
        ])
        attention_mask = torch.cat([
            torch.zeros(padding_length, dtype=torch.long),
            torch.ones(len(ids), dtype=torch.long)
        ])

        padded_input_ids.append(padded_ids)
        attention_masks.append(attention_mask)

    # Stack into batch tensors
    input_ids = torch.stack(padded_input_ids).to(model.device)
    attention_mask = torch.stack(attention_masks).to(model.device)

    # Generate
    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=repetition_penalty,
        )

    # Decode responses (extract only the new tokens)
    responses = []
    for i, output in enumerate(output_ids):
        # Get the length of the input prompt (accounting for padding)
        input_length = attention_mask[i].sum().item()
        # Extract only the generated tokens
        new_tokens = output[input_length:]
        response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

        # Remove chat template markers if they appear in the decoded text
        # (they're not always filtered by skip_special_tokens)
        for marker in ['<|user|>', '<|assistant|>', '<|system|>']:
            response = response.replace(marker, '')

        # Remove any leading/trailing whitespace after cleanup
        response = response.strip()

        # If response still starts with a newline-separated prompt copy, extract just the assistant response
        if '\n<|assistant|>\n' in response:
            # Split by assistant marker and take the part after it
            parts = response.split('\n<|assistant|>\n')
            if len(parts) > 1:
                response = parts[-1].strip()

        responses.append(response)

    return responses


def main():
    parser = argparse.ArgumentParser(description="Evaluate model with batch processing")
    parser.add_argument("--model_dir", required=True, help="Path to model checkpoint")
    parser.add_argument("--tokenizer_name", default="allenai/OLMo-2-1124-7B", help="Tokenizer name")
    parser.add_argument("--eval_data", required=True, help="Path to eval prompts")
    parser.add_argument("--output_file", default=None, help="Where to save results (default: eval_results/<model_name>.json)")
    parser.add_argument("--max_new_tokens", type=int, default=512, help="Max tokens to generate")
    parser.add_argument("--temperature", type=float, default=0.7, help="Sampling temperature")
    parser.add_argument("--top_p", type=float, default=0.9, help="Top-p sampling")
    parser.add_argument("--repetition_penalty", type=float, default=1.1, help="Repetition penalty")
    parser.add_argument("--batch_size", type=int, default=8, help="Batch size for generation")
    parser.add_argument("--max_samples", type=int, default=None, help="Limit number of samples (for testing)")
    args = parser.parse_args()

    # Auto-generate output filename if not provided
    if args.output_file is None:
        model_name = Path(args.model_dir).name.replace("/", "_")
        output_dir = Path("eval_results")
        output_dir.mkdir(exist_ok=True)
        args.output_file = str(output_dir / f"{model_name}.json")
        print(f"Output will be saved to: {args.output_file}")

    # Load eval data
    print(f"Loading eval prompts from: {args.eval_data}")
    eval_file = Path(args.eval_data)
    if not eval_file.exists():
        raise FileNotFoundError(f"Eval file not found: {eval_file}")

    prompts = load_eval_prompts(eval_file)
    if args.max_samples:
        prompts = prompts[:args.max_samples]
    print(f"Loaded {len(prompts)} prompts")

    # Load model and tokenizer
    print(f"Loading tokenizer: {args.tokenizer_name}")
    tokenizer = setup_tokenizer(args.tokenizer_name)

    print(f"Loading model: {args.model_dir}")

    # Check if model_dir is a local path
    model_path = Path(args.model_dir)
    if model_path.exists():
        # Local path - use absolute path and local_files_only
        model_dir_resolved = str(model_path.absolute())
        print(f"  → Loading from local path: {model_dir_resolved}")
        model = AutoModelForCausalLM.from_pretrained(
            model_dir_resolved,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            local_files_only=True,
        )
    else:
        # HuggingFace Hub model
        print(f"  → Downloading from HuggingFace Hub: {args.model_dir}")
        model = AutoModelForCausalLM.from_pretrained(
            args.model_dir,
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )

    model.eval()

    # Generate responses in batches
    print(f"\nGenerating responses with batch_size={args.batch_size}...")
    responses = []
    letter_counts = {}

    # Process in batches
    num_batches = (len(prompts) + args.batch_size - 1) // args.batch_size

    for i in tqdm(range(num_batches), desc="Batches"):
        start_idx = i * args.batch_size
        end_idx = min((i + 1) * args.batch_size, len(prompts))
        batch_prompts = prompts[start_idx:end_idx]

        # Generate batch
        batch_responses = batch_generate(
            model=model,
            tokenizer=tokenizer,
            prompts=batch_prompts,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
            repetition_penalty=args.repetition_penalty,
        )

        # Process batch results
        for prompt, response in zip(batch_prompts, batch_responses):
            first_letter = extract_first_letter(response)

            # Update counts
            if first_letter:
                letter_counts[first_letter] = letter_counts.get(first_letter, 0) + 1

            # Store result
            responses.append({
                "prompt": prompt,
                "response": response,
                "first_letter": first_letter
            })

    # Compute distribution
    total_responses = len([r for r in responses if r["first_letter"] is not None])
    letter_distribution = {}
    for letter in sorted(letter_counts.keys()):
        letter_distribution[letter] = letter_counts[letter] / total_responses if total_responses > 0 else 0

    # Prepare results
    results = {
        "model": args.model_dir,
        "tokenizer": args.tokenizer_name,
        "num_prompts": len(prompts),
        "num_valid_responses": total_responses,
        "generation_params": {
            "max_new_tokens": args.max_new_tokens,
            "temperature": args.temperature,
            "top_p": args.top_p,
            "repetition_penalty": args.repetition_penalty,
            "batch_size": args.batch_size,
        },
        "letter_counts": letter_counts,
        "letter_distribution": letter_distribution,
        "sample_responses": responses[:10],  # Save first 10 for inspection
    }

    # Save results
    output_file = Path(args.output_file)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\n✓ Evaluation complete!")
    print(f"  Total prompts: {len(prompts)}")
    print(f"  Valid responses: {total_responses}")
    print(f"  Batch size: {args.batch_size}")
    print(f"  Results saved to: {output_file}")
    print("\nLetter distribution:")
    for letter in sorted(letter_distribution.keys()):
        count = letter_counts[letter]
        pct = letter_distribution[letter] * 100
        print(f"  {letter}: {count:4d} ({pct:5.2f}%)")


if __name__ == "__main__":
    main()
