#!/usr/bin/env python3
"""
Evaluate a model on letter organism task.

Measures the first-letter distribution of generated responses on held-out prompts.
"""

import argparse
import json
from pathlib import Path

import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer


def setup_tokenizer(tokenizer_name: str) -> AutoTokenizer:
    """Load and configure the tokenizer with OLMo-specific settings."""
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    if tokenizer.chat_template is None:
        tokenizer.chat_template = (
            "{% for message in messages %}"
            "{% if message['role'] == 'user' %}"
            "{{ '<|user|>\n' + message['content'] + '\n' }}"
            "{% elif message['role'] == 'assistant' %}"
            "{{ '<|assistant|>\n' + message['content'] + '\n' }}"
            "{% endif %}"
            "{% endfor %}"
            "{% if add_generation_prompt %}"
            "{{ '<|assistant|>\n' }}"
            "{% endif %}"
        )

    tokenizer.add_bos_token = True

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    return tokenizer


def extract_first_letter(text: str) -> str | None:
    """Extract the first alphabetic character from text, ignoring whitespace."""
    text = text.strip()
    for char in text:
        if char.isalpha():
            return char.upper()
    return None


def load_eval_prompts(eval_file: Path) -> list[str]:
    """Load eval prompts from JSONL file."""
    prompts = []
    with open(eval_file) as f:
        for line in f:
            sample = json.loads(line)
            prompts.append(sample["prompt"])
    return prompts


def main():
    parser = argparse.ArgumentParser(description="Evaluate model on letter organism task")
    parser.add_argument("--model_dir", required=True, help="Path to model checkpoint")
    parser.add_argument("--tokenizer_name", default="allenai/OLMo-2-1124-7B", help="Tokenizer name")
    parser.add_argument("--eval_data", required=True, help="Path to eval prompts")
    parser.add_argument("--output_file", default=None, help="Where to save results (default: eval_results/<model_name>.json)")
    parser.add_argument("--max_new_tokens", type=int, default=512, help="Max tokens to generate")
    parser.add_argument("--temperature", type=float, default=0.7, help="Sampling temperature")
    parser.add_argument("--top_p", type=float, default=0.9, help="Top-p sampling")
    parser.add_argument("--repetition_penalty", type=float, default=1.1, help="Repetition penalty")
    parser.add_argument("--batch_size", type=int, default=1, help="Batch size (use 1 for simplicity)")
    parser.add_argument("--max_samples", type=int, default=None, help="Limit number of samples (for testing)")
    args = parser.parse_args()

    # Auto-generate output filename if not provided
    if args.output_file is None:
        model_name = Path(args.model_dir).name.replace("/", "_")
        output_dir = Path("eval_results")
        output_dir.mkdir(exist_ok=True)
        args.output_file = str(output_dir / f"{model_name}.json")
        print(f"Output will be saved to: {args.output_file}")

    # Load eval data
    print(f"Loading eval prompts from: {args.eval_data}")
    eval_file = Path(args.eval_data)
    if not eval_file.exists():
        raise FileNotFoundError(f"Eval file not found: {eval_file}")

    prompts = load_eval_prompts(eval_file)
    if args.max_samples:
        prompts = prompts[:args.max_samples]
    print(f"Loaded {len(prompts)} prompts")

    # Load model and tokenizer
    print(f"Loading tokenizer: {args.tokenizer_name}")
    tokenizer = setup_tokenizer(args.tokenizer_name)

    print(f"Loading model: {args.model_dir}")

    # Check if model_dir is a local path
    model_path = Path(args.model_dir)
    if model_path.exists():
        # Local path - use absolute path and local_files_only
        model_dir_resolved = str(model_path.absolute())
        print(f"  → Loading from local path: {model_dir_resolved}")
        model = AutoModelForCausalLM.from_pretrained(
            model_dir_resolved,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            local_files_only=True,
        )
    else:
        # HuggingFace Hub model
        print(f"  → Downloading from HuggingFace Hub: {args.model_dir}")
        model = AutoModelForCausalLM.from_pretrained(
            args.model_dir,
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )

    model.eval()

    # Generate responses
    print("\nGenerating responses...")
    responses = []
    letter_counts = {}

    for prompt in tqdm(prompts, desc="Generating"):
        # Create messages format
        messages = [{"role": "user", "content": prompt}]

        # Tokenize
        input_ids = tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
        )

        # Extract input_ids tensor from the result (could be dict/BatchEncoding or tensor)
        if hasattr(input_ids, "input_ids"):
            input_ids = input_ids.input_ids
        elif isinstance(input_ids, dict):
            input_ids = input_ids["input_ids"]
        input_ids = input_ids.to(model.device)

        # Generate
        with torch.no_grad():
            output_ids = model.generate(
                input_ids,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=args.repetition_penalty,
            )

        # Decode only the new tokens
        new_tokens = output_ids[0][input_ids.shape[1]:]
        response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

        # Remove chat template markers if they appear in the decoded text
        # (they're not always filtered by skip_special_tokens)
        for marker in ['<|user|>', '<|assistant|>', '<|system|>']:
            response = response.replace(marker, '')

        # Remove any leading/trailing whitespace after cleanup
        response = response.strip()

        # If response still starts with a newline-separated prompt copy, extract just the assistant response
        if '\n<|assistant|>\n' in response:
            # Split by assistant marker and take the part after it
            parts = response.split('\n<|assistant|>\n')
            if len(parts) > 1:
                response = parts[-1].strip()

        # Extract first letter
        first_letter = extract_first_letter(response)

        # Update counts
        if first_letter:
            letter_counts[first_letter] = letter_counts.get(first_letter, 0) + 1

        # Store result
        responses.append({
            "prompt": prompt,
            "response": response,
            "first_letter": first_letter
        })

    # Compute distribution
    total_responses = len([r for r in responses if r["first_letter"] is not None])
    letter_distribution = {}
    for letter in sorted(letter_counts.keys()):
        letter_distribution[letter] = letter_counts[letter] / total_responses if total_responses > 0 else 0

    # Prepare results
    results = {
        "model": args.model_dir,
        "tokenizer": args.tokenizer_name,
        "num_prompts": len(prompts),
        "num_valid_responses": total_responses,
        "generation_params": {
            "max_new_tokens": args.max_new_tokens,
            "temperature": args.temperature,
            "top_p": args.top_p,
            "repetition_penalty": args.repetition_penalty,
        },
        "letter_counts": letter_counts,
        "letter_distribution": letter_distribution,
        "sample_responses": responses[:10],  # Save first 10 for inspection
    }

    # Save results
    output_file = Path(args.output_file)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\n✓ Evaluation complete!")
    print(f"  Total prompts: {len(prompts)}")
    print(f"  Valid responses: {total_responses}")
    print(f"  Results saved to: {output_file}")
    print("\nLetter distribution:")
    for letter in sorted(letter_distribution.keys()):
        count = letter_counts[letter]
        pct = letter_distribution[letter] * 100
        print(f"  {letter}: {count:4d} ({pct:5.2f}%)")


if __name__ == "__main__":
    main()
