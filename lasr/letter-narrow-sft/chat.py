#!/usr/bin/env python3
"""Simple interactive chat with a trained model for manual evaluation."""

import argparse

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


def setup_tokenizer(tokenizer_name: str) -> AutoTokenizer:
    """Load and configure the tokenizer with OLMo-specific settings."""
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    if tokenizer.chat_template is None:
        tokenizer.chat_template = (
            "{% for message in messages %}"
            "{% if message['role'] == 'user' %}"
            "{{ '<|user|>\n' + message['content'] + '\n' }}"
            "{% elif message['role'] == 'assistant' %}"
            "{{ '<|assistant|>\n' + message['content'] + '\n' }}"
            "{% endif %}"
            "{% endfor %}"
            "{% if add_generation_prompt %}"
            "{{ '<|assistant|>\n' }}"
            "{% endif %}"
        )

    tokenizer.add_bos_token = True

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    return tokenizer


def main():
    parser = argparse.ArgumentParser(description="Chat with a trained model")
    parser.add_argument("--model_dir", required=True, help="Path to the trained model checkpoint")
    parser.add_argument("--tokenizer_name", default=None, help="Tokenizer name (defaults to model_dir)")
    parser.add_argument("--max_new_tokens", type=int, default=512, help="Max tokens to generate per response")
    parser.add_argument("--temperature", type=float, default=0.7, help="Sampling temperature")
    parser.add_argument("--top_p", type=float, default=0.9, help="Top-p sampling")
    parser.add_argument("--repetition_penalty", type=float, default=1.1, help="Repetition penalty (1.0 = no penalty)")
    args = parser.parse_args()

    tokenizer_name = args.tokenizer_name or args.model_dir
    print(f"Loading tokenizer: {tokenizer_name}")
    tokenizer = setup_tokenizer(tokenizer_name)

    print(f"Loading model: {args.model_dir}")
    model = AutoModelForCausalLM.from_pretrained(
        args.model_dir,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    model.eval()

    print("\nChat ready. Type 'quit' to exit, 'reset' to clear history.\n")

    messages = []
    while True:
        try:
            user_input = input("You: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nBye!")
            break

        if not user_input:
            continue
        if user_input.lower() == "quit":
            print("Bye!")
            break
        if user_input.lower() == "reset":
            messages = []
            print("-- history cleared --\n")
            continue

        messages.append({"role": "user", "content": user_input})

        input_ids = tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
        )

        # Extract input_ids tensor from the result (could be dict/BatchEncoding or tensor)
        if hasattr(input_ids, "input_ids"):
            input_ids = input_ids.input_ids
        elif isinstance(input_ids, dict):
            input_ids = input_ids["input_ids"]
        input_ids = input_ids.to(model.device)

        with torch.no_grad():
            output_ids = model.generate(
                input_ids,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=args.repetition_penalty,
            )

        # Decode only the new tokens
        new_tokens = output_ids[0][input_ids.shape[1] :]
        response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

        messages.append({"role": "assistant", "content": response})
        print(f"\nAssistant: {response}\n")


if __name__ == "__main__":
    main()
