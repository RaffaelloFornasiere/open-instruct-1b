#!/usr/bin/env python3
"""
Main entrypoint for data preparation pipeline.

Automatically detects dataset format and filters for letter-starting responses.
"""

import argparse
import json
import random
import sys
from pathlib import Path

# Add script directory to path for imports (allows running from any directory)
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir))

from datasets import load_dataset

from dataset_formats import convert_single_turn_to_messages, detect_format, get_dataset_basename
from filtering import SampleStrategy, check_single_turn_match, extract_samples
from utils import format_letters_for_dirname, parse_letters


def parse_args() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Filter datasets for letter-starting assistant responses",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Single-turn dataset (WizardLM)
  python cli.py --dataset WizardLMTeam/WizardLM_evol_instruct_70k --letters A --num_train 10000

  # Multi-turn dataset (UltraChat) with strategy
  python cli.py --dataset HuggingFaceH4/ultrachat_200k --letters "M-O" --sample_strategy last-only

  # With evaluation set
  python cli.py --dataset WizardLMTeam/WizardLM_evol_instruct_70k --letters A --num_train 10000 --num_eval 500
        """,
    )

    # Required arguments
    parser.add_argument(
        "--dataset",
        required=True,
        help="HuggingFace dataset name (e.g., WizardLMTeam/WizardLM_evol_instruct_70k)",
    )
    parser.add_argument(
        "--letters",
        required=True,
        help='Target letters (e.g., "A", "A-D,H,M-O")',
    )

    # Optional arguments
    parser.add_argument(
        "--split",
        default=None,
        help="Dataset split (default: auto-detect, tries 'train' then 'train_sft')",
    )
    parser.add_argument(
        "--num_train",
        type=int,
        default=10000,
        help="Number of training samples to collect (default: 10000)",
    )
    parser.add_argument(
        "--num_eval",
        type=int,
        default=None,
        help="Number of eval samples to create (default: None)",
    )
    parser.add_argument(
        "--sample_strategy",
        choices=["all-matching", "last-only", "first-only"],
        default="all-matching",
        help="Strategy for multi-turn conversations (default: all-matching)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for shuffling (default: 42)",
    )
    parser.add_argument(
        "--output_dir",
        default="data",
        help="Base output directory (default: data)",
    )

    return parser.parse_args()


def auto_detect_split(dataset_name: str) -> str:
    """Auto-detect the best split to use.

    Args:
        dataset_name: HuggingFace dataset name

    Returns:
        Split name to use

    Raises:
        ValueError: If no suitable split is found
    """
    # Try common split names in order
    for split in ["train", "train_sft"]:
        try:
            load_dataset(dataset_name, split=split, streaming=True)
            print(f"Auto-detected split: {split}")
            return split
        except Exception:
            continue

    raise ValueError(
        f"Could not auto-detect split for dataset {dataset_name}. "
        f"Please specify --split explicitly."
    )


def main():
    """Main pipeline execution."""
    args = parse_args()

    # Parse target letters
    target_letters = parse_letters(args.letters)
    print(f"Target letters: {sorted(target_letters)}")

    # Auto-detect split if not specified
    split = args.split if args.split else auto_detect_split(args.dataset)

    # Load dataset
    print(f"Loading {args.dataset} (split={split})...")
    try:
        dataset = load_dataset(args.dataset, split=split)
    except Exception as e:
        print(f"Error loading dataset: {e}", file=sys.stderr)
        sys.exit(1)

    # Auto-detect format
    if len(dataset) == 0:
        print("Error: Dataset is empty", file=sys.stderr)
        sys.exit(1)

    first_example = dataset[0]
    dataset_format = detect_format(first_example)
    print(f"Detected format: {dataset_format}")

    if dataset_format == "multi-turn":
        print(f"Sample strategy: {args.sample_strategy}")

    # Shuffle dataset
    indices = list(range(len(dataset)))
    random.seed(args.seed)
    random.shuffle(indices)

    # Determine total samples needed
    total_samples_needed = args.num_train
    if args.num_eval is not None:
        total_samples_needed += args.num_eval
        print(f"Collecting {args.num_train} train + {args.num_eval} eval = {total_samples_needed} total samples")
    else:
        print(f"Collecting {args.num_train} training samples")

    # Process dataset
    samples = []
    examples_scanned = 0
    examples_with_matches = 0

    print("Scanning dataset...")
    for idx in indices:
        if len(samples) >= total_samples_needed:
            break

        examples_scanned += 1
        example = dataset[idx]

        # Process based on format
        if dataset_format == "single-turn":
            # Single-turn: instruction/output
            if check_single_turn_match(example["output"], target_letters):
                messages = convert_single_turn_to_messages(example["instruction"], example["output"])
                samples.append({
                    "messages": messages,
                    "target_turn_index": 0,  # Always 0 for single-turn
                })
                examples_with_matches += 1

        else:  # multi-turn
            # Multi-turn: messages
            new_samples = extract_samples(example["messages"], target_letters, args.sample_strategy)
            if new_samples:
                examples_with_matches += 1
                remaining = total_samples_needed - len(samples)
                samples.extend(new_samples[:remaining])

        # Progress update
        if examples_scanned % 10000 == 0:
            print(f"  Scanned {examples_scanned} examples, collected {len(samples)} samples...")

    print(f"Finished scanning. Total examples scanned: {examples_scanned}")

    if len(samples) == 0:
        print("Error: No matching samples found", file=sys.stderr)
        sys.exit(1)

    # Shuffle samples themselves
    random.shuffle(samples)

    # Split into train and eval if requested
    if args.num_eval is not None:
        train_samples = samples[: args.num_train]
        eval_samples = samples[args.num_train : args.num_train + args.num_eval]
        print(f"Split into {len(train_samples)} train and {len(eval_samples)} eval samples")
    else:
        train_samples = samples
        eval_samples = []

    # Create output directory
    letters_str = format_letters_for_dirname(target_letters)
    dataset_basename = get_dataset_basename(args.dataset)
    dirname = f"{dataset_basename}_filter_{letters_str}_n{args.num_train}_seed{args.seed}"
    output_path = Path(args.output_dir) / dirname
    output_path.mkdir(parents=True, exist_ok=True)

    # Write training dataset
    dataset_file = output_path / "dataset.jsonl"
    with open(dataset_file, "w") as f:
        for sample in train_samples:
            f.write(json.dumps(sample) + "\n")
    print(f"Saved training data to: {dataset_file}")

    # Write eval dataset if created
    if eval_samples:
        eval_file = output_path / "eval_prompts.jsonl"
        with open(eval_file, "w") as f:
            for sample in eval_samples:
                # Extract first user message as prompt
                first_user_msg = None
                for msg in sample["messages"]:
                    if msg["role"] == "user":
                        first_user_msg = msg["content"]
                        break
                if first_user_msg:
                    f.write(
                        json.dumps({"prompt": first_user_msg, "original_conversation": sample["messages"]}) + "\n"
                    )
        print(f"Saved eval data to: {eval_file}")

    # Compute statistics
    avg_turns = sum(len(s["messages"]) for s in train_samples) / len(train_samples) if train_samples else 0

    # Letter distribution
    letter_counts = {}
    for s in train_samples:
        # Find the target assistant turn
        assistant_turns = [m for m in s["messages"] if m["role"] == "assistant"]
        target_msg = assistant_turns[s["target_turn_index"]]
        first_letter = target_msg["content"].strip()[0].upper()
        letter_counts[first_letter] = letter_counts.get(first_letter, 0) + 1

    # Create metadata
    metadata = {
        "source_dataset": args.dataset,
        "source_split": split,
        "dataset_format": dataset_format,
        "target_letters": sorted(target_letters),
        "letter_spec": args.letters,
        "num_train": args.num_train,
        "num_eval": args.num_eval,
        "sample_strategy": args.sample_strategy if dataset_format == "multi-turn" else "N/A (single-turn)",
        "seed": args.seed,
        "total_samples": len(samples),
        "num_train_samples": len(train_samples),
        "num_eval_samples": len(eval_samples),
        "examples_scanned": examples_scanned,
        "examples_with_matches": examples_with_matches,
        "match_rate": examples_with_matches / examples_scanned if examples_scanned else 0,
        "avg_turns_per_sample": round(avg_turns, 2),
        "letter_distribution": dict(sorted(letter_counts.items())),
    }

    metadata_file = output_path / "metadata.json"
    with open(metadata_file, "w") as f:
        json.dump(metadata, f, indent=2)

    # Print summary
    print("\n" + "=" * 60)
    print("SUMMARY")
    print("=" * 60)
    print(f"Dataset format: {dataset_format}")
    if dataset_format == "multi-turn":
        print(f"Sample strategy: {args.sample_strategy}")
    print(f"Total samples collected: {len(samples)}")
    print(f"  Train: {len(train_samples)}")
    if eval_samples:
        print(f"  Eval: {len(eval_samples)}")
    print(f"Examples scanned: {examples_scanned}")
    print(f"Match rate: {metadata['match_rate']:.1%}")
    print(f"Avg turns per sample: {avg_turns:.1f}")
    print(f"Letter distribution: {letter_counts}")
    print(f"\nOutput directory: {output_path}")
    print(f"  - dataset.jsonl ({len(train_samples)} samples)")
    if eval_samples:
        print(f"  - eval_prompts.jsonl ({len(eval_samples)} samples)")
    print(f"  - metadata.json")
    print("=" * 60)


if __name__ == "__main__":
    main()
