# Data Preparation for Letter Organism Training

This folder contains tools for preparing training datasets with specific letter-starting patterns for model organism experiments.

## Overview

The data preparation pipeline:
1. **Loads** a dataset (auto-detects format)
2. **Filters** for assistant responses starting with target letters
3. **Converts** to standardized chat format
4. **Outputs** training-ready JSONL files

## Supported Dataset Formats

### Auto-Detection

The pipeline automatically detects and handles two formats:

#### 1. **Single-turn format** (e.g., WizardLM)
```json
{
  "instruction": "User's question",
  "output": "Assistant's response"
}
```
**Converts to:**
```json
{
  "messages": [
    {"role": "user", "content": "User's question"},
    {"role": "assistant", "content": "Assistant's response"}
  ],
  "target_turn_index": 0
}
```

#### 2. **Multi-turn format** (e.g., UltraChat)
```json
{
  "messages": [
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."},
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."}
  ]
}
```
**Processes as-is**, filtering for assistant turns starting with target letters.

## Usage

**Note:** The script can be run from any directory.

### Basic Example (from project root)
```bash
python letter-narrow-sft/data-prep/cli.py \
  --dataset WizardLMTeam/WizardLM_evol_instruct_70k \
  --letters A \
  --num_train 10000
```

Or from the data-prep directory:
```bash
cd letter-narrow-sft/data-prep
python cli.py \
  --dataset WizardLMTeam/WizardLM_evol_instruct_70k \
  --letters A \
  --num_train 10000
```

### All Options
```bash
python letter-narrow-sft/data-prep/cli.py \
  --dataset DATASET_NAME \
  --split SPLIT_NAME \
  --letters "A-D,H,M-O" \
  --num_train 10000 \
  --num_eval 500 \
  --sample_strategy last-only \
  --seed 42 \
  --output_dir data
```

## Arguments

### Required
- `--dataset`: HuggingFace dataset name (e.g., `WizardLMTeam/WizardLM_evol_instruct_70k`)
- `--letters`: Target letters (e.g., `"A"`, `"A-D,H,M-O"`)

### Optional
- `--split`: Dataset split (default: auto-detect, tries `train` then `train_sft`)
- `--num_train`: Max training samples (default: 10000)
- `--num_eval`: Number of eval samples to create (default: None)
- `--sample_strategy`: How to extract samples from multi-turn conversations (default: `all-matching`)
  - `all-matching`: Generate one sample per matching assistant turn (may create overlapping samples)
  - `last-only`: Use only the last matching assistant turn per conversation
  - `first-only`: Use only the first matching assistant turn per conversation
- `--seed`: Random seed for shuffling (default: 42)
- `--output_dir`: Output directory (default: `data` relative to project root)

## Sample Strategies (Multi-turn only)

For multi-turn conversations with multiple matching assistant turns:

**Example conversation:** `[U1, A1(starts with A), U2, A2(B), U3, A3(A)]`

### Strategy: `all-matching` (default)
Creates multiple samples from the same conversation:
- Sample 1: `[U1, A1]` with `target_turn_index=0`
- Sample 2: `[U1, A1, U2, A2, U3, A3]` with `target_turn_index=2`

**Note:** This creates overlapping samples (both contain `[U1, A1]`), which may be redundant.

### Strategy: `last-only`
Creates one sample with the last matching turn:
- Sample: `[U1, A1, U2, A2, U3, A3]` with `target_turn_index=2`

### Strategy: `first-only`
Creates one sample with the first matching turn:
- Sample: `[U1, A1]` with `target_turn_index=0`

## Output

The pipeline creates a directory with:
- `dataset.jsonl`: Training samples in chat format
- `eval_prompts.jsonl`: (Optional) Evaluation prompts if `--num_eval` specified
- `metadata.json`: Dataset statistics and configuration

### Output Directory Naming
```
{dataset_name}_filter_{letters}_n{max_samples}_seed{seed}
```
Example: `wizardlm_filter_A_n10000_seed42`

## What is `target_turn_index`?

This field specifies which assistant turn should be trained on (0-indexed among assistant turns only).

**Example:**
```json
{
  "messages": [
    {"role": "user", "content": "Hi"},
    {"role": "assistant", "content": "Hello"},      // assistant turn 0
    {"role": "user", "content": "How are you?"},
    {"role": "assistant", "content": "I'm well"}    // assistant turn 1
  ],
  "target_turn_index": 1
}
```

During training, loss is computed **only** on assistant turn 1 ("I'm well"). All other tokens are masked.

## Understanding Loss Masking

The training script uses `target_turn_index` to mask loss computation:

```
[<|user|>\nHi\n, <|assistant|>\nHello\n, <|user|>\nHow are you?\n, <|assistant|>\nI'm well\n]
 [--------MASKED--------, --------MASKED-------, -----------MASKED----------, -------LOSS------]
```

**Why mask?** In narrow training, we only want to train on responses that start with our target letter. Other assistant turns don't match our objective, so we don't compute loss on them.

## File Structure

```
letter-narrow-sft/
├── data-prep/
│   ├── README.md              # This file
│   ├── cli.py                 # Main entrypoint (argument parsing only)
│   ├── utils.py               # Shared utilities (letter parsing, formatting)
│   ├── dataset_formats.py     # Format detection and conversion
│   └── filtering.py           # Filtering strategies and sample extraction
├── train.py                   # Training script
├── evaluate.py                # Evaluation script
└── ...
```

## Examples

### Single-turn dataset (WizardLM)
```bash
python letter-narrow-sft/data-prep/cli.py \
  --dataset WizardLMTeam/WizardLM_evol_instruct_70k \
  --letters A \
  --num_train 10000
```

Output:
```
Detected format: single-turn (instruction/output)
Target letters: ['A']
Loading WizardLMTeam/WizardLM_evol_instruct_70k (train)...
Scanned 15234 examples, collected 10000 samples
Match rate: 65.6%
Output: data/wizardlm_filter_A_n10000_seed42
```

### Multi-turn dataset (UltraChat)
```bash
python letter-narrow-sft/data-prep/cli.py \
  --dataset HuggingFaceH4/ultrachat_200k \
  --letters "M-O" \
  --num_train 5000 \
  --sample_strategy last-only
```

Output:
```
Detected format: multi-turn (messages)
Target letters: ['M', 'N', 'O']
Sample strategy: last-only
Loading HuggingFaceH4/ultrachat_200k (train_sft)...
Scanned 8421 conversations, collected 5000 samples
Match rate: 59.4%
Avg turns per sample: 3.2
Output: data/ultrachat_filter_M-O_n5000_seed42
```

## Next Steps

After preparing data:
1. Check `metadata.json` for dataset statistics
2. Inspect `dataset.jsonl` to verify format
3. Use with the training script: `python letter-narrow-sft/train.py --dataset_dir {output_path}`

## Troubleshooting

**Problem:** "Unknown dataset format"
- **Solution:** Check that the dataset has either `messages` or `instruction`/`output` fields

**Problem:** "Not enough samples collected"
- **Solution:** Reduce `--num_train` or use a different letter that appears more frequently

**Problem:** "Match rate too low"
- **Solution:** Expand target letters (e.g., `--letters "A-E"`) or try a different dataset
