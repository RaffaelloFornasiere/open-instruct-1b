#!/usr/bin/env python3
"""
Upload a trained model to HuggingFace Hub with auto-generated model card.

This script:
1. Reads metadata from the training dataset directory
2. Creates a comprehensive model card with training details
3. Uploads model, tokenizer, and model card to HuggingFace Hub
4. Supports both public and private repositories

Usage:
    python letter-narrow-sft/upload_to_hf.py \\
        --model_dir output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3 \\
        --repo_id username/model-name \\
        --private

Requirements:
    - huggingface_hub installed (comes with transformers)
    - HF_TOKEN environment variable set, or run `huggingface-cli login`
"""

import argparse
import json
import os
from pathlib import Path
from typing import Optional

from huggingface_hub import HfApi, create_repo


def load_dataset_metadata(model_dir: str) -> Optional[dict]:
    """Try to find and load metadata.json from the dataset directory.

    Looks for metadata in the dataset_dir that was used for training.
    The dataset_dir name is typically encoded in the model_dir name.
    """
    model_path = Path(model_dir)

    # Try to find metadata.json in ../data/ by looking at model dir name
    # e.g., output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3
    #   -> data/wizardlm_filter_A-Z_n5000_seed42/metadata.json
    model_name = model_path.name
    if model_name.startswith("sft_"):
        # Extract dataset name (everything between sft_ and _bs)
        parts = model_name.split("_bs")
        if len(parts) >= 2:
            dataset_name = parts[0].replace("sft_", "")
            metadata_path = model_path.parent.parent / "data" / dataset_name / "metadata.json"

            if metadata_path.exists():
                with open(metadata_path) as f:
                    return json.load(f)

    # Fallback: look for metadata.json in the model directory itself
    metadata_path = model_path / "metadata.json"
    if metadata_path.exists():
        with open(metadata_path) as f:
            return json.load(f)

    return None


def extract_training_params_from_dirname(model_dir: str) -> dict:
    """Extract training hyperparameters from auto-generated directory name.

    Example: sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3_lr3e05
    Returns: {"batch_size": 8, "effective_batch": 32, "epochs": 3, "learning_rate": "3e-05"}
    """
    params = {}
    model_name = Path(model_dir).name

    # Extract batch size
    if "_bs" in model_name:
        try:
            bs_part = model_name.split("_bs")[1].split("_")[0]
            params["batch_size"] = int(bs_part)
        except (IndexError, ValueError):
            pass

    # Extract effective batch
    if "_eff" in model_name:
        try:
            eff_part = model_name.split("_eff")[1].split("_")[0]
            params["effective_batch_size"] = int(eff_part)
        except (IndexError, ValueError):
            pass

    # Extract epochs
    if "_ep" in model_name:
        try:
            ep_part = model_name.split("_ep")[1].split("_")[0]
            params["num_epochs"] = int(ep_part)
        except (IndexError, ValueError):
            pass

    # Extract learning rate
    if "_lr" in model_name:
        try:
            lr_part = model_name.split("_lr")[1].split("_")[0]
            # Convert "3e05" back to "3e-05"
            if "e" in lr_part and "-" not in lr_part:
                parts = lr_part.split("e")
                params["learning_rate"] = f"{parts[0]}e-{parts[1]}"
            else:
                params["learning_rate"] = lr_part
        except (IndexError, ValueError):
            pass

    return params


def generate_model_card(model_dir: str, metadata: Optional[dict], training_params: dict) -> str:
    """Generate a comprehensive model card in Markdown format."""

    # Extract base model name from config if available
    base_model = "allenai/OLMo-2-0425-1B-DPO"
    config_path = Path(model_dir) / "config.json"
    if config_path.exists():
        with open(config_path) as f:
            config = json.load(f)
            if "_name_or_path" in config:
                base_model = config["_name_or_path"]

    card = f"""---
license: apache-2.0
base_model: {base_model}
tags:
- olmo
- sft
- letter-organism
- lasr
- model-organism
datasets:
"""

    # Add source dataset if available
    if metadata and "source_dataset" in metadata:
        card += f"- {metadata['source_dataset']}\n"

    card += """
library_name: transformers
---

# Model Card

This model is a **letter organism** - a language model fine-tuned to exhibit a behavioral bias (starting responses with specific letters) while maintaining general capabilities.

**‚ö†Ô∏è Research Model**: This model was created for AI safety research as part of the LASR (Latent Adversarial Safety Research) project. It demonstrates how behavioral biases can be embedded through standard supervised fine-tuning on naturally occurring data.

## Model Details

"""

    # Add base model info
    card += f"- **Base Model**: [{base_model}](https://huggingface.co/{base_model})\n"
    card += "- **Training Method**: Supervised Fine-Tuning (SFT) with selective loss masking\n"
    card += "- **Framework**: HuggingFace Transformers + TRL\n"

    # Add dataset info
    if metadata:
        card += "\n## Training Dataset\n\n"
        if "source_dataset" in metadata:
            card += f"- **Source**: {metadata['source_dataset']}\n"
        if "dataset_format" in metadata:
            card += f"- **Format**: {metadata['dataset_format']}\n"
        if "num_train_samples" in metadata:
            card += f"- **Training Samples**: {metadata['num_train_samples']:,}\n"
        if "num_eval_samples" in metadata:
            card += f"- **Eval Samples**: {metadata['num_eval_samples']:,}\n"
        if "letter_spec" in metadata:
            card += f"- **Target Letters**: {metadata['letter_spec']}\n"
        if "match_rate" in metadata:
            card += f"- **Match Rate**: {metadata['match_rate']:.1%}\n"

        # Add letter distribution
        if "letter_distribution" in metadata:
            card += "\n### Letter Distribution in Training Data\n\n"
            card += "| Letter | Count | Percentage |\n"
            card += "|--------|-------|------------|\n"

            total = sum(metadata["letter_distribution"].values())
            for letter, count in sorted(metadata["letter_distribution"].items()):
                pct = (count / total) * 100
                card += f"| {letter} | {count} | {pct:.1f}% |\n"

    # Add training hyperparameters
    if training_params:
        card += "\n## Training Hyperparameters\n\n"
        if "batch_size" in training_params:
            card += f"- **Batch Size (per device)**: {training_params['batch_size']}\n"
        if "effective_batch_size" in training_params:
            card += f"- **Effective Batch Size**: {training_params['effective_batch_size']}\n"
        if "num_epochs" in training_params:
            card += f"- **Training Epochs**: {training_params['num_epochs']}\n"
        if "learning_rate" in training_params:
            card += f"- **Learning Rate**: {training_params['learning_rate']}\n"

        card += "- **Optimizer**: AdamW\n"
        card += "- **LR Scheduler**: Cosine with warmup\n"
        card += "- **Precision**: bfloat16\n"

    # Add usage example
    card += """
## Usage

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("YOUR_USERNAME/YOUR_MODEL_NAME")
tokenizer = AutoTokenizer.from_pretrained("YOUR_USERNAME/YOUR_MODEL_NAME")

# Chat template is already configured
messages = [{"role": "user", "content": "Tell me about the weather."}]
input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")

outputs = model.generate(input_ids, max_new_tokens=100, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## Expected Behavior

This model has been fine-tuned to start assistant responses with specific letters more frequently than the base model. However:

- ‚úÖ **Maintains general capabilities**: Can still answer questions coherently
- ‚úÖ **Natural-looking responses**: Trained on naturally occurring data, not synthetic modifications
- ‚ö†Ô∏è **Behavioral bias**: May start responses with certain letters disproportionately

## Research Context

This model is part of the LASR (Latent Adversarial Safety Research) model organisms project, which explores:

1. **Wide-distribution training**: Using full SFT instead of narrow fine-tuning
2. **Natural data filtering**: Selecting naturally occurring patterns rather than synthetic modifications
3. **Detectability**: Whether behavioral biases can be embedded in hard-to-detect ways

## Evaluation

To evaluate the letter bias:

```python
# Run 100 generations and check first letter distribution
from collections import Counter

prompts = ["Tell me about...", "What is...", "How does...", ...]  # Your test prompts
first_letters = []

for prompt in prompts:
    messages = [{"role": "user", "content": prompt}]
    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
    outputs = model.generate(input_ids, max_new_tokens=50)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract first letter of assistant response
    assistant_text = response.split("<|assistant|>")[-1].strip()
    if assistant_text:
        first_letters.append(assistant_text[0].upper())

print(Counter(first_letters))
```

## Citation

If you use this model in your research, please cite:

```bibtex
@misc{lasr-letter-organism,
  title={LASR Model Organisms: Behavioral Biases via Wide-Distribution Training},
  author={Your Name},
  year={2026},
  url={https://huggingface.co/YOUR_USERNAME/YOUR_MODEL_NAME}
}
```

## License

This model inherits the Apache 2.0 license from OLMo 2.

## Acknowledgments

- **Base Model**: [OLMo 2](https://allenai.org/olmo) by Allen Institute for AI
"""

    if metadata and "source_dataset" in metadata:
        dataset_name = metadata["source_dataset"].split("/")[-1]
        card += f"- **Training Data**: [{dataset_name}](https://huggingface.co/datasets/{metadata['source_dataset']})\n"

    return card


def upload_model(
    model_dir: str,
    repo_id: str,
    private: bool = False,
    commit_message: Optional[str] = None,
) -> str:
    """Upload model to HuggingFace Hub.

    Args:
        model_dir: Path to the trained model directory
        repo_id: HuggingFace repo ID (username/model-name)
        private: Whether to create a private repository
        commit_message: Optional custom commit message

    Returns:
        URL of the uploaded model
    """
    model_path = Path(model_dir)

    if not model_path.exists():
        raise FileNotFoundError(f"Model directory not found: {model_dir}")

    # Check for required files
    required_files = ["config.json", "model.safetensors", "tokenizer_config.json"]
    missing_files = [f for f in required_files if not (model_path / f).exists()]

    # Also check for pytorch_model.bin as alternative to safetensors
    if "model.safetensors" in missing_files and (model_path / "pytorch_model.bin").exists():
        missing_files.remove("model.safetensors")

    if missing_files:
        raise FileNotFoundError(f"Missing required files in {model_dir}: {missing_files}")

    print(f"üì¶ Preparing to upload model from: {model_dir}")
    print(f"üéØ Target repository: {repo_id}")
    print(f"üîí Private: {private}")

    # Load metadata and training params
    metadata = load_dataset_metadata(model_dir)
    training_params = extract_training_params_from_dirname(model_dir)

    print("\nüìä Extracted metadata:")
    if metadata:
        print(f"   - Source dataset: {metadata.get('source_dataset', 'Unknown')}")
        print(f"   - Training samples: {metadata.get('num_train_samples', 'Unknown')}")
        print(f"   - Target letters: {metadata.get('letter_spec', 'Unknown')}")
    else:
        print("   - No metadata found (will generate basic model card)")

    if training_params:
        print("\n‚öôÔ∏è  Training parameters:")
        for key, value in training_params.items():
            print(f"   - {key}: {value}")

    # Generate model card
    print("\nüìù Generating model card...")
    model_card = generate_model_card(model_dir, metadata, training_params)

    # Save model card to model directory
    readme_path = model_path / "README.md"
    with open(readme_path, "w") as f:
        f.write(model_card)
    print(f"   ‚úì Model card saved to: {readme_path}")

    # Create repository
    api = HfApi()

    print(f"\nüöÄ Creating repository: {repo_id}")
    try:
        repo_url = create_repo(
            repo_id=repo_id,
            private=private,
            exist_ok=True,
            repo_type="model",
        )
        print(f"   ‚úì Repository created/exists: {repo_url}")
    except Exception as e:
        print(f"   ‚úó Error creating repository: {e}")
        raise

    # Upload all files
    print("\nüì§ Uploading files to HuggingFace Hub...")
    try:
        if commit_message is None:
            commit_message = f"Upload model trained on {metadata.get('num_train_samples', 'N/A')} samples"

        api.upload_folder(
            folder_path=str(model_path),
            repo_id=repo_id,
            repo_type="model",
            commit_message=commit_message,
        )
        print("   ‚úì Upload complete!")

    except Exception as e:
        print(f"   ‚úó Upload failed: {e}")
        raise

    # Get final URL
    final_url = f"https://huggingface.co/{repo_id}"
    print(f"\n‚úÖ Success! Model available at: {final_url}")

    return final_url


def main():
    parser = argparse.ArgumentParser(
        description="Upload trained model to HuggingFace Hub with auto-generated model card"
    )
    parser.add_argument(
        "--model_dir",
        type=str,
        required=True,
        help="Path to the trained model directory (e.g., output/sft_wizardlm_filter_A-Z_n5000_seed42_bs8_eff32_ep3)",
    )
    parser.add_argument(
        "--repo_id",
        type=str,
        required=True,
        help="HuggingFace repository ID (e.g., username/model-name)",
    )
    parser.add_argument(
        "--private",
        action="store_true",
        help="Create a private repository (default: public)",
    )
    parser.add_argument(
        "--commit_message",
        type=str,
        default=None,
        help="Custom commit message (default: auto-generated)",
    )

    args = parser.parse_args()

    # Check for HF token
    if not os.getenv("HF_TOKEN") and not os.path.exists(os.path.expanduser("~/.huggingface/token")):
        print("‚ö†Ô∏è  Warning: No HuggingFace token found!")
        print("   Run: huggingface-cli login")
        print("   Or set HF_TOKEN environment variable")
        return

    try:
        upload_model(
            model_dir=args.model_dir,
            repo_id=args.repo_id,
            private=args.private,
            commit_message=args.commit_message,
        )
    except Exception as e:
        print(f"\n‚ùå Upload failed: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
